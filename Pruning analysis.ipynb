{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "from hyperopt import STATUS_OK, hp, fmin, tpe, Trials, space_eval\n",
    "\n",
    "from time import time\n",
    "import operator\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "def load_data():\n",
    "    full_data = pd.read_csv(\"X.csv\")\n",
    "    train_y = pd.read_csv(\"ytr.csv\")\n",
    "    # Rename columns to something more interpretable\n",
    "    columns = ([\"reflectance_\" + str(i) for i in range(7)]\n",
    "               + [\"solar_\" + str(i) for i in range(5)] + [\"id\"])\n",
    "    full_data.columns = columns\n",
    "    # Add y to the data frame\n",
    "    split = 98000\n",
    "    y_id_dict = train_y.set_index(\"Id\")[\"y\"].to_dict()\n",
    "    full_data.loc[:(split-1), \"y\"] = full_data.loc[:(split-1), \"id\"].map(y_id_dict)\n",
    "\n",
    "    train, test = full_data[:split], full_data[split:]\n",
    "    return (train, test)\n",
    "\n",
    "#columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    " #          + [\"solar_\" + str(i) for i in range(5)] + [\"y\"])\n",
    "#full_data = pd.read_csv(\"MODIS.csv\", header=None, names=columns)\n",
    "#split = 98000\n",
    "#train, test = full_data[:split].copy(), full_data[split:].copy()\n",
    "#train = full_data.copy()\n",
    "\n",
    "#train_copy, test_copy = load_data()\n",
    "\n",
    "train, test = load_data()\n",
    "\n",
    "# Parameters\n",
    "n_threads = -1\n",
    "random_seed = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reflectance_0</th>\n",
       "      <th>reflectance_1</th>\n",
       "      <th>reflectance_2</th>\n",
       "      <th>reflectance_3</th>\n",
       "      <th>reflectance_4</th>\n",
       "      <th>reflectance_5</th>\n",
       "      <th>reflectance_6</th>\n",
       "      <th>solar_0</th>\n",
       "      <th>solar_1</th>\n",
       "      <th>solar_2</th>\n",
       "      <th>solar_3</th>\n",
       "      <th>solar_4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.088535</td>\n",
       "      <td>0.050097</td>\n",
       "      <td>0.007748</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.052130</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.150211</td>\n",
       "      <td>0.091345</td>\n",
       "      <td>0.062856</td>\n",
       "      <td>0.140568</td>\n",
       "      <td>0.076832</td>\n",
       "      <td>0.032414</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.041447</td>\n",
       "      <td>0.276798</td>\n",
       "      <td>0.089301</td>\n",
       "      <td>0.072769</td>\n",
       "      <td>0.237950</td>\n",
       "      <td>0.109721</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.029073</td>\n",
       "      <td>0.027024</td>\n",
       "      <td>0.088950</td>\n",
       "      <td>0.052317</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  reflectance_0  reflectance_1  reflectance_2  reflectance_3  \\\n",
       "0   1       0.026993       0.012067       0.088535       0.050097   \n",
       "1   1       0.029457       0.019613       0.087705       0.052130   \n",
       "2   1       0.038491       0.150211       0.091345       0.062856   \n",
       "3   1       0.041447       0.276798       0.089301       0.072769   \n",
       "4   1       0.029073       0.027024       0.088950       0.052317   \n",
       "\n",
       "   reflectance_4  reflectance_5  reflectance_6    solar_0     solar_1  \\\n",
       "0       0.007748       0.004051       0.002929  28.420588  146.782941   \n",
       "1       0.016930       0.010574       0.003654  28.420588  146.782941   \n",
       "2       0.140568       0.076832       0.032414  28.420588  146.782941   \n",
       "3       0.237950       0.109721       0.036960  28.420588  146.782941   \n",
       "4       0.021162       0.011535       0.005997  28.420588  146.782941   \n",
       "\n",
       "     solar_2     solar_3     solar_4         y  \n",
       "0  20.686471  100.594706  159.884706  0.075627  \n",
       "1  20.686471  100.594706  159.884706  0.075627  \n",
       "2  20.686471  100.594706  159.884706  0.075627  \n",
       "3  20.686471  100.594706  159.884706  0.075627  \n",
       "4  20.686471  100.594706  159.884706  0.075627  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Outlier 1 data linear\n",
    "#columns = [\"id\", \"x\", \"y\"]\n",
    "#full_data = pd.read_csv(\"outliers_1_nonlinear.csv\", header=None,\n",
    " #                       names=columns).sort_values(by=\"id\").reset_index(drop=True)\n",
    "#split = 10000\n",
    "#train, test = full_data[:split].copy(), full_data[split:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_excl = [\"id\", \"y\"]\n",
    "cols_orig = [c for c in train.columns if c not in cols_excl]\n",
    "\n",
    "# Remove outliers\n",
    "#outliers_id = [21, 31, 72, 85, 135, 154, 165,\n",
    " #              199, 232, 252, 255, 262, 289, 387,\n",
    "  #             393, 404, 408, 434, 488, 516, 578,\n",
    "   #            615, 617, 624, 633, 647, 683, 778,\n",
    "    #           785, 792, 817, 828, 917, 946, 960]\n",
    "\n",
    "#train = train[~train[\"id\"].isin(outliers_id)].reset_index(drop=True)\n",
    "\n",
    "# Standardise data for LR\n",
    "train[cols_orig] = scale(train[cols_orig])\n",
    "test[cols_orig] = scale(test[cols_orig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pdf_weight(data):\n",
    "    cols = list(data.columns)\n",
    "    X = data.copy()\n",
    "    \n",
    "    y_mean_dict = X.groupby(\"id\")[\"y_hat\"].mean().to_dict()\n",
    "    y_std_dict = X.groupby(\"id\")[\"y_hat\"].std().to_dict()\n",
    "    X[\"y_hat_mean\"] = X[\"id\"].map(y_mean_dict)\n",
    "    X[\"y_hat_std\"] = X[\"id\"].map(y_std_dict)\n",
    "    X[\"pdf\"] = norm.pdf(X[\"y_hat\"], X[\"y_hat_mean\"], \n",
    "                        X[\"y_hat_std\"])\n",
    "    y_pdf_sum_dict = X.groupby(\"id\")[\"pdf\"].sum().to_dict()\n",
    "    X[\"pdf_sum\"] = X[\"id\"].map(y_pdf_sum_dict)\n",
    "    X[\"pdf\"] /= X[\"pdf_sum\"]\n",
    "    X[\"y_hat_weighted\"] = X[\"y_hat\"] * X[\"pdf\"]\n",
    "    \n",
    "    y_weighted_dict = X.groupby(\"id\")[\"y_hat_weighted\"].sum().to_dict()\n",
    "    X[\"y_hat_weighted_sum\"] = X[\"id\"].map(y_weighted_dict)\n",
    "    \n",
    "    return(X[cols + [\"y_hat_weighted_sum\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "models_weights = {\"dnn_1\": 1}#, \"dnn_2\": 0.2, \"dnn_3\": 0.2,\n",
    "                  #\"dnn_4\": 0.2, \"dnn_5\": 0.2}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "#models_cols = {\"dnn_1\": cols_dnn, \"dnn_2\": cols_dnn, \"dnn_3\": cols_dnn,\n",
    " #              \"dnn_4\": cols_dnn, \"dnn_5\": cols_dnn}\n",
    "    \n",
    "#learning_rate = 0.1\n",
    "activation_fn = tf.nn.relu6\n",
    "\n",
    "# Scoring function in the hyperopt hyperparameters tuning.\n",
    "def scoring_function(parameters):\n",
    "    print(\"Training the model with parameters: \")\n",
    "    print(parameters)\n",
    "    average_RMSE = 0.0\n",
    "    n_splits = 5\n",
    "    \n",
    "    # Generate random integer for model_dir\n",
    "    random_int = np.random.randint(1000)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    nb_fold = 0\n",
    "    for train_index, validation_index in kf.split(train):\n",
    "        nb_fold += 1\n",
    "        train_fold, validation_fold = train.loc[train_index], train.loc[validation_index]\n",
    "        \n",
    "        # Remove outliers\n",
    "        #train_fold = train_fold[~train_fold[\"id\"].isin(outliers_id)].reset_index(drop=True)\n",
    "\n",
    "        feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "        count = 0\n",
    "        model_dir = (\"./log_\"\n",
    "                     + str(parameters[\"steps\"]) + \"_\"\n",
    "                     + str(parameters[\"nb_neurons_1\"]) + \"_\"\n",
    "                     #+ str(parameters[\"nb_neurons_2\"])\n",
    "                     + str(nb_fold) + \"_\"\n",
    "                     + str(count) + \"_\"\n",
    "                     + str(random_int)\n",
    "                    )\n",
    "        \n",
    "        # Tune number of layers\n",
    "        model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                  hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                #parameters[\"nb_neurons_2\"]],\n",
    "                                                  #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                   #   learning_rate=learning_rate,\n",
    "                                                    #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                  #dropout=parameters[\"dropout\"],\n",
    "                                                  activation_fn=activation_fn,\n",
    "                                                  model_dir=model_dir)\n",
    "\n",
    "        def input_fn(data_set):\n",
    "            feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "            labels = tf.constant(data_set[\"y\"].values)\n",
    "            return feature_cols, labels\n",
    "        \n",
    "        model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "        train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "        #for i, m in models.items():\n",
    "        temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "        # .predict() returns an iterator; convert to an array\n",
    "        y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "        train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "        # Use median value by id\n",
    "        y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "        RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train_fold[\"y\"]))\n",
    "        \n",
    "        # Prune outliers\n",
    "        RMSE_decreasing = True\n",
    "        while (RMSE_decreasing):\n",
    "            count +=1\n",
    "            train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "            # Distance from the median for each bag\n",
    "            # Changed to distance from real target value\n",
    "            train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "            # Rank of each instance by bag\n",
    "            train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "            bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "            train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "            train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "            # Remove outliers\n",
    "            outliers_index = train_pred[\"rank\"] > (1 - parameters[\"outliers_threshold\"])\n",
    "            train_fold = train_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "            \n",
    "            model_dir = (\"./log_\"\n",
    "                         + str(parameters[\"steps\"]) + \"_\"\n",
    "                         + str(parameters[\"nb_neurons_1\"]) + \"_\"\n",
    "                         #+ str(parameters[\"nb_neurons_2\"])\n",
    "                         + str(nb_fold) + \"_\"\n",
    "                         + str(count) + \"_\"\n",
    "                         + str(random_int)\n",
    "                        )\n",
    "\n",
    "            model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                      hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                    #parameters[\"nb_neurons_2\"]],\n",
    "                                                      #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                       #   learning_rate=learning_rate,\n",
    "                                                        #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                      #dropout=parameters[\"dropout\"],\n",
    "                                                      activation_fn=activation_fn,\n",
    "                                                      model_dir=model_dir)\n",
    "\n",
    "            model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "            # Compute new RMSE\n",
    "            train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            #for i, m in models.items():\n",
    "            temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "            y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "            new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train_fold[\"y\"]))\n",
    "            print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "            \n",
    "            if (abs(new_RMSE - RMSE) > parameters[\"gain_threshold\"]):\n",
    "            # 5 iterations of pruning\n",
    "            #if (count < 5):\n",
    "                RMSE = new_RMSE\n",
    "            else:\n",
    "                RMSE_decreasing = False\n",
    "        \n",
    "        # Bagging of RNN\n",
    "        # Bootstrap 1\n",
    "        #train_fold_1 = train_fold\n",
    "        #model_dnn_1 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                     #   learning_rate=learning_rate,\n",
    "                                                      #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                    #dropout=parameters[\"dropout\"])\n",
    "        #model_dnn_1.fit(input_fn=lambda: input_fn(train_fold_1), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Boostrap 2\n",
    "        #train_fold_2 = resample(train_fold, random_state=random_seed).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_2 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_2.fit(input_fn=lambda: input_fn(train_fold_2), steps=parameters[\"steps\"])\n",
    "            \n",
    "        # Bootstrap 3\n",
    "        #train_fold_3 = resample(train_fold, random_state=(random_seed+1)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_3 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_3.fit(input_fn=lambda: input_fn(train_fold_3), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Bootstrap 4\n",
    "        #train_fold_4 = resample(train_fold, random_state=(random_seed+2)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_4 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_4.fit(input_fn=lambda: input_fn(train_fold_4), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Bootstrap 5\n",
    "        #train_fold_5 = resample(train_fold, random_state=(random_seed+3)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_5 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_5.fit(input_fn=lambda: input_fn(train_fold_5), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Changed to model_dnn instead of model_dnn_1\n",
    "        models = {\"dnn_1\": model_dnn}#, \"dnn_2\": model_dnn_2, \"dnn_3\": model_dnn_3,\n",
    "                  #\"dnn_4\": model_dnn_4, \"dnn_5\": model_dnn_5}\n",
    "        \n",
    "        # Compute RMSE on validation set\n",
    "        validation_pred = validation_fold[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "        for i, m in models.items():\n",
    "            temp = m.predict(input_fn=lambda: input_fn(validation_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            validation_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "            \n",
    "        # Weight each instance by gaussian pdf\n",
    "        #validation_pred = pdf_weight(validation_pred)\n",
    "        # Use median value by id\n",
    "        y_hat_med = validation_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "        \n",
    "        #validation_pred[\"y_hat_weighted_sum\"]\n",
    "        #validation_fold[\"id\"].map(y_hat_med).values\n",
    "        RMSE = np.sqrt(mean_squared_error(validation_fold[\"id\"].map(y_hat_med).values, validation_fold[\"y\"]))\n",
    "        average_RMSE += RMSE\n",
    "        print(\"Validation fold {0} RMSE: {1}\".format(nb_fold, RMSE))\n",
    "\n",
    "    average_RMSE /= n_splits\n",
    "\n",
    "    print(\"Cross-validation score: {0}\\n\".format(average_RMSE))\n",
    "    \n",
    "    return {\"loss\": average_RMSE, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 1000, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
      "Pruning 1 RMSE: 0.719296250944\n",
      "Validation fold 1 RMSE: 0.692832710495\n",
      "Pruning 1 RMSE: 0.702423933706\n",
      "Pruning 2 RMSE: 0.6790789557\n",
      "Pruning 3 RMSE: 0.681096149794\n",
      "Validation fold 2 RMSE: 0.735890554455\n",
      "Pruning 1 RMSE: 0.695173105402\n",
      "Validation fold 3 RMSE: 0.738100628681\n",
      "Pruning 1 RMSE: 0.707885361611\n",
      "Pruning 2 RMSE: 0.705337373934\n",
      "Validation fold 4 RMSE: 0.723181765471\n",
      "Pruning 1 RMSE: 0.729587263229\n",
      "Pruning 2 RMSE: 0.693925977725\n",
      "Pruning 3 RMSE: 0.720382095763\n",
      "Pruning 4 RMSE: 0.753779903316\n",
      "Pruning 5 RMSE: 0.692342825664\n",
      "Pruning 6 RMSE: 0.682007701585\n",
      "Pruning 7 RMSE: 0.67891242487\n",
      "Validation fold 5 RMSE: 0.774067681983\n",
      "Cross-validation score: 0.732814668217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Grid to pick parameters from.\n",
    "parameters_grid = {\"steps\"             : hp.choice(\"steps\", np.arange(1000, 1100, 100, dtype=int)),\n",
    "                   \"nb_neurons_1\"      : hp.choice(\"nb_neurons_1\", np.arange(9, 10, 1, dtype=int)),\n",
    "                   \"outliers_threshold\": hp.quniform(\"outliers_threshold\", 0.05, 0.051, 0.01),\n",
    "                   \"gain_threshold\"    : hp.quniform(\"gain_threshold\", 0.01, 0.015, 0.005)\n",
    "                   #\"dropout\": hp.quniform(\"dropout\", 0.2, 0.4, 0.1)\n",
    "                   #\"l2_reg\": hp.quniform(\"l2_reg\", 0.00, 0.005, 0.01)\n",
    "                   #\"nb_neurons_2\": hp.choice(\"nb_neurons_2\", np.arange(5, 10, 1, dtype=int))\n",
    "                  }\n",
    "# Record the information about the cross-validation.\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(scoring_function, parameters_grid, algo=tpe.suggest, max_evals=1, \n",
    "            trials=trials)\n",
    "\n",
    "computing_time = time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 DNN with relu_6 and gaussian weights\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 1000, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.719296250944\n",
    "Validation fold 1 RMSE: 0.692832710495\n",
    "Pruning 1 RMSE: 0.702423933706\n",
    "Pruning 2 RMSE: 0.6790789557\n",
    "Pruning 3 RMSE: 0.681096149794\n",
    "Validation fold 2 RMSE: 0.735890554455\n",
    "Pruning 1 RMSE: 0.695173105402\n",
    "Validation fold 3 RMSE: 0.738100628681\n",
    "Pruning 1 RMSE: 0.707885361611\n",
    "Pruning 2 RMSE: 0.705337373934\n",
    "Validation fold 4 RMSE: 0.723181765471\n",
    "Pruning 1 RMSE: 0.729587263229\n",
    "Pruning 2 RMSE: 0.693925977725\n",
    "Pruning 3 RMSE: 0.720382095763\n",
    "Pruning 4 RMSE: 0.753779903316\n",
    "Pruning 5 RMSE: 0.692342825664\n",
    "Pruning 6 RMSE: 0.682007701585\n",
    "Pruning 7 RMSE: 0.67891242487\n",
    "Validation fold 5 RMSE: 0.774067681983\n",
    "Cross-validation score: 0.732814668217\n",
    "    \n",
    "1 DNN with relu_6 activation function\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 1000, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.708234883916\n",
    "Pruning 2 RMSE: 0.691853914852\n",
    "Pruning 3 RMSE: 0.698473758017\n",
    "Validation fold 1 RMSE: 0.646112355893\n",
    "Pruning 1 RMSE: 0.694808562274\n",
    "Validation fold 2 RMSE: 0.748946370502\n",
    "Pruning 1 RMSE: 0.698011334158\n",
    "Pruning 2 RMSE: 0.708838516152\n",
    "Validation fold 3 RMSE: 0.731923862826\n",
    "Pruning 1 RMSE: 0.75324816444\n",
    "Pruning 2 RMSE: 0.734924713078\n",
    "Pruning 3 RMSE: 0.706555052743\n",
    "Pruning 4 RMSE: 0.7143481095\n",
    "Validation fold 4 RMSE: 0.73727659496\n",
    "Pruning 1 RMSE: 0.696562960464\n",
    "Validation fold 5 RMSE: 0.793280237684\n",
    "Cross-validation score: 0.731507884373\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "1 DNN\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 900, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.720018452178\n",
    "Pruning 2 RMSE: 0.694637808863\n",
    "Pruning 3 RMSE: 0.682604548885\n",
    "Pruning 4 RMSE: 0.675240927353\n",
    "Validation fold 1 RMSE: 0.657330001629\n",
    "Pruning 1 RMSE: 0.67466681128\n",
    "Pruning 2 RMSE: 0.65951277037\n",
    "Pruning 3 RMSE: 0.649785148888\n",
    "Validation fold 2 RMSE: 0.748604123648\n",
    "Pruning 1 RMSE: 0.68920733019\n",
    "Pruning 2 RMSE: 0.673928761686\n",
    "Pruning 3 RMSE: 0.663794898173\n",
    "Pruning 4 RMSE: 0.656852425765\n",
    "Validation fold 3 RMSE: 0.75238232566\n",
    "Pruning 1 RMSE: 0.695945822399\n",
    "Pruning 2 RMSE: 0.677436347483\n",
    "Pruning 3 RMSE: 0.667360192164\n",
    "Pruning 4 RMSE: 0.661270048569\n",
    "Validation fold 4 RMSE: 0.703611985649\n",
    "Pruning 1 RMSE: 0.675491954494\n",
    "Pruning 2 RMSE: 0.654833366872\n",
    "Pruning 3 RMSE: 0.641692836958\n",
    "Pruning 4 RMSE: 0.633366415513\n",
    "Validation fold 5 RMSE: 0.784596896669\n",
    "Cross-validation score: 0.729305066651\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MODIS train: 980 bags\n",
    "    \n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 1000, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.119078992959\n",
    "Pruning 2 RMSE: 0.112813390542\n",
    "Pruning 3 RMSE: 0.110762151709\n",
    "Pruning 4 RMSE: 0.116923346022\n",
    "Pruning 5 RMSE: 0.110191410888\n",
    "Validation fold 1 RMSE: 0.102889475987\n",
    "Pruning 1 RMSE: 0.109445820186\n",
    "Pruning 2 RMSE: 0.11020136135\n",
    "Pruning 3 RMSE: 0.102425573921\n",
    "Pruning 4 RMSE: 0.0970353939936\n",
    "Pruning 5 RMSE: 0.107031912134\n",
    "Validation fold 2 RMSE: 0.126743592916\n",
    "Pruning 1 RMSE: 0.119576965904\n",
    "Pruning 2 RMSE: 0.116071040853\n",
    "Pruning 3 RMSE: 0.112926603658\n",
    "Pruning 4 RMSE: 0.103269803892\n",
    "Pruning 5 RMSE: 0.119983564288\n",
    "Validation fold 3 RMSE: 0.137163785473\n",
    "Pruning 1 RMSE: 0.119435904777\n",
    "Pruning 2 RMSE: 0.104272554095\n",
    "Pruning 3 RMSE: 0.107650999677\n",
    "Pruning 4 RMSE: 0.102014944828\n",
    "Pruning 5 RMSE: 0.106218193445\n",
    "Validation fold 4 RMSE: 0.0962407158912\n",
    "Pruning 1 RMSE: 0.116800784396\n",
    "Pruning 2 RMSE: 0.123153155007\n",
    "Pruning 3 RMSE: 0.108245060392\n",
    "Pruning 4 RMSE: 0.111636782652\n",
    "Pruning 5 RMSE: 0.1039692717\n",
    "Validation fold 5 RMSE: 0.165833779616\n",
    "Cross-validation score: 0.125774269977\n",
    "    \n",
    "    \n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 900, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.105343361018\n",
    "Validation fold 1 RMSE: 0.0987015791108\n",
    "Pruning 1 RMSE: 0.100121169819\n",
    "Pruning 2 RMSE: 0.0939525682587\n",
    "Validation fold 2 RMSE: 0.139847916988\n",
    "Pruning 1 RMSE: 0.121320922084\n",
    "Pruning 2 RMSE: 0.111532420256\n",
    "Validation fold 3 RMSE: 0.13168961676\n",
    "Pruning 1 RMSE: 0.10949595882\n",
    "Pruning 2 RMSE: 0.103120828612\n",
    "Validation fold 4 RMSE: 0.107883013046\n",
    "Pruning 1 RMSE: 0.112995428702\n",
    "Validation fold 5 RMSE: 0.188379683693\n",
    "Cross-validation score: 0.13330036192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Outliers linear\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 1000, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.0804155030185\n",
    "Pruning 2 RMSE: 0.0794809993989\n",
    "Pruning 3 RMSE: 0.065860152061\n",
    "Pruning 4 RMSE: 0.061603047503\n",
    "Pruning 5 RMSE: 0.0586138764275\n",
    "Validation fold 1 RMSE: 0.0617678059642\n",
    "Pruning 1 RMSE: 0.0843172240725\n",
    "Pruning 2 RMSE: 0.0748873140784\n",
    "Pruning 3 RMSE: 0.065482659712\n",
    "Pruning 4 RMSE: 0.0613644908284\n",
    "Pruning 5 RMSE: 0.0609045015971\n",
    "Validation fold 2 RMSE: 0.0673808018742\n",
    "Pruning 1 RMSE: 0.0811678935522\n",
    "Pruning 2 RMSE: 0.0772371483952\n",
    "Pruning 3 RMSE: 0.0660389858725\n",
    "Pruning 4 RMSE: 0.0614534539826\n",
    "Pruning 5 RMSE: 0.059306412431\n",
    "Validation fold 3 RMSE: 0.0600865575504\n",
    "Pruning 1 RMSE: 0.0814326476057\n",
    "Pruning 2 RMSE: 0.0719821617032\n",
    "Pruning 3 RMSE: 0.0645202690828\n",
    "Pruning 4 RMSE: 0.0598876604168\n",
    "Pruning 5 RMSE: 0.0593301500914\n",
    "Validation fold 4 RMSE: 0.0648812016439\n",
    "Pruning 1 RMSE: 0.0810715252891\n",
    "Pruning 2 RMSE: 0.0745169133733\n",
    "Pruning 3 RMSE: 0.064487828201\n",
    "Pruning 4 RMSE: 0.0617649969488\n",
    "Pruning 5 RMSE: 0.0601317270497\n",
    "Validation fold 5 RMSE: 0.0624430532392\n",
    "Cross-validation score: 0.0633118840544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MODIS train: 1364 bags\n",
    "    \n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 900, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.119817931796\n",
    "Validation fold 1 RMSE: 0.111377722376\n",
    "Pruning 1 RMSE: 0.112396098948\n",
    "Pruning 2 RMSE: 0.101808581135\n",
    "Validation fold 2 RMSE: 0.148817162914\n",
    "Pruning 1 RMSE: 0.115222994126\n",
    "Pruning 2 RMSE: 0.108615616905\n",
    "Validation fold 3 RMSE: 0.110455499827\n",
    "Pruning 1 RMSE: 0.111609881409\n",
    "Validation fold 4 RMSE: 0.158362010857\n",
    "Pruning 1 RMSE: 0.108284906653\n",
    "Validation fold 5 RMSE: 0.120915922243\n",
    "Cross-validation score: 0.129985663643\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6523287945244072"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(trials.losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the best parameters as a csv.\n",
    "best_parameters = pd.DataFrame({key: [value] for (key, value) in \n",
    "                                zip(space_eval(parameters_grid, best).keys(),\n",
    "                                    space_eval(parameters_grid, best).values())})\n",
    "# Add the corresponding score.\n",
    "best_parameters[\"score\"] = min(trials.losses())\n",
    "best_parameters.to_csv(\"best_parameters_7.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 1 RMSE: 0.117672757803\n",
      "Pruning 2 RMSE: 0.124796130382\n",
      "Pruning 3 RMSE: 0.10612483743\n",
      "Pruning 4 RMSE: 0.10924961052\n",
      "Pruning 5 RMSE: 0.104698510927\n"
     ]
    }
   ],
   "source": [
    "cols_dnn = cols_orig\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Prediction of test for each iteration of pruning\n",
    "y_test_hat = []\n",
    "\n",
    "count = 0\n",
    "random_nb = np.random.randint(1000)\n",
    "model_dir = \"./pruning_\" + str(count) + \"_\" + str(random_nb)\n",
    "\n",
    "# Aggregation\n",
    "#train = train.groupby(\"id\").mean().reset_index()\n",
    "#test = test.groupby(\"id\").mean().reset_index()\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "# Tune number of layers\n",
    "model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                          hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                          model_dir=model_dir)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "        \n",
    "train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to an array\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train[\"y\"]))\n",
    "\n",
    "test_pred = test[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "temp = model_dnn.predict(input_fn=lambda: input_fn(test))\n",
    "# .predict() returns an iterator; convert to an array\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "test_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med_test = test_pred.groupby(\"id\").median()[\"y_hat\"]\n",
    "y_test_hat.append(y_hat_med_test)\n",
    "\n",
    "## prune outliers code\n",
    "RMSE_decreasing = True\n",
    "while (RMSE_decreasing):\n",
    "    count += 1\n",
    "    train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "    # Distance from the median for each bag\n",
    "    train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "    # Rank of each instance by bag\n",
    "    train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "    bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "    train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "    train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    outliers_index = train_pred[\"rank\"] > (1 - best_parameters[\"outliers_threshold\"][0])\n",
    "    train = train.loc[~outliers_index, :].reset_index(drop=True)\n",
    "    \n",
    "    model_dir = \"./pruning_\" + str(count) + \"_\" + str(random_nb)\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "    # Use median value by id\n",
    "    y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "    new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train[\"y\"]))\n",
    "    print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "    \n",
    "    test_pred = test[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(test))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    test_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "    # Use median value by id\n",
    "    y_hat_med_test = test_pred.groupby(\"id\").median()[\"y_hat\"]\n",
    "    y_test_hat.append(y_hat_med_test)\n",
    "\n",
    "    #if (abs(new_RMSE - RMSE) > best_parameters[\"gain_threshold\"][0]):\n",
    "    if (count < 5):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        RMSE_decreasing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.137282745074\n",
      "0.156872448576\n",
      "0.157716406352\n",
      "0.169905028537\n",
      "0.136445378764\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(np.sqrt(mean_squared_error(y_test_hat[i].values, test.groupby(\"id\")[\"y\"].median().values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "10 neurons 1000 steps MODIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = np.arange(0, 1.01, 0.01)\n",
    "plt.scatter(test[\"x\"], test[\"y\"], s=0.5)\n",
    "plt.plot(grid, grid**2, color=\"green\")\n",
    "plt.scatter(test[\"x\"], test_pred[\"y_hat\"], color=\"red\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pred = test[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "for i, m in models.items():\n",
    "    temp = m.predict(input_fn=lambda: input_fn(test))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    test_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = test_pred.groupby(\"id\").median()[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71423164615804291"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(test_pred[\"id\"].map(y_hat_med).values, test[\"y\"]))\n",
    "RMSE\n",
    "#0.65725435012348465 for Pred 4\n",
    "#0.65362864377856866 for Pred 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark:\n",
    "* Submit 1 (ensemble of xgboost + 2 ridge with instances model)\n",
    "eta\teval_metric\tgamma\tlambda\tmax_depth\tmin_child_weight\tnthread\tobjective\tseed\tsilent\tsubsample\tscore\n",
    "0.91834 Public LB 300 trees 0.09\trmse\t0.2\t0.8\t4\t4.0\t-1\treg:linear\t22\t0\t0.7\t0.883339 (cross-val)\n",
    "    \n",
    "* Submit 2\n",
    "\n",
    "Pruning with linear regression\n",
    "then add contributions of aggregated xgboost + linear model\n",
    "\n",
    "0.78181 Public LB  0.779345 CV\n",
    "\n",
    "* Submit 3\n",
    "DNN pruning\n",
    "(wrong CV)\n",
    "0.73270 Public LB 0.663128 CV with DNN 10 neurons 900 steps gain_threshold = 0.01 outliers_threshold = 0.05\n",
    "\n",
    "* Prediction 4\n",
    "LB 0.74713\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.701658836521\n",
    "using fist pruning DNN\n",
    "* Prediction 5\n",
    "LB 0.74453\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.01, 'steps': 2400, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.707300896236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Dropout\n",
    "L2 regu\n",
    "Validation set instead of cross val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gamma_i_j(j, pi_i, X_i, y_i, delta):\n",
    "    out = 0.0\n",
    "    out = pi_i[j] * norm.pdf(y_i, X_i[j], delta)\n",
    "    out /= sum(pi_i * norm.pdf(y_i, X_i, delta))\n",
    "    return(out)\n",
    "\n",
    "def EM_Q(pi, X, y, delta):\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "cols_dnn = cols_orig\n",
    "\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "\n",
    "model_dir_f = \"./log_submit_6\"\n",
    "\n",
    "# Fit DNN regressor\n",
    "model_dnn_f = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                            model_dir=model_dir_f)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn_f.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "# Train prediction\n",
    "train_pred = train[[\"id\"]].assign(y_hat=0, pi_hat=0)\n",
    "temp = model_dnn_f.predict(input_fn=lambda: input_fn(train))\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Fit DNN softmax\n",
    "model_dir_g = \"./g_log_EM\"\n",
    "model_dnn_g = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_g\"][0]],\n",
    "                                            model_dir=model_dir_g)\n",
    "\n",
    "model_dnn_g.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_g\"][0])\n",
    "temp = model_dnn_g.predict(input_fn=lambda: input_fn(train))\n",
    "pi_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "\n",
    "# Compute softmax\n",
    "train_pred[\"pi_hat\"] = np.exp(pi_hat)\n",
    "pi_hat_sum_dict = train_pred.groupby(\"id\")[\"pi_hat\"].sum().to_dict()\n",
    "# \"map\" is actually much faster than \"replace\"\n",
    "train_pred[\"pi_hat_sum\"] = train_pred[\"id\"].map(pi_hat_sum_dict)\n",
    "train_pred[\"pi_hat\"] /= train_pred[\"pi_hat_sum\"]\n",
    "\n",
    "# EM algorithm\n",
    "# Change to Q\n",
    "EM_decreasing = True\n",
    "nb_iteration = 0\n",
    "while (EM_decreasing):\n",
    "    nb_iteration +=1\n",
    "\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "\n",
    "    print(\"Iteration {0} EM: {1}\".format(nb_iteration, new_RMSE))\n",
    "\n",
    "    if (abs(new_RMSE - RMSE) > 0.1):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        EM_decreasing = False\n",
    "\n",
    "models = {\"dnn_1\": model_dnn}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
