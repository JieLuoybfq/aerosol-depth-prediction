{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "from hyperopt import STATUS_OK, hp, fmin, tpe, Trials, space_eval\n",
    "\n",
    "from time import time\n",
    "import operator\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "def load_data():\n",
    "    full_data = pd.read_csv(\"X.csv\")\n",
    "    train_y = pd.read_csv(\"ytr.csv\")\n",
    "    # Rename columns to something more interpretable\n",
    "    columns = ([\"reflectance_\" + str(i) for i in range(7)]\n",
    "               + [\"solar_\" + str(i) for i in range(5)] + [\"id\"])\n",
    "    full_data.columns = columns\n",
    "    # Add y to the data frame\n",
    "    split = 98000\n",
    "    y_id_dict = train_y.set_index(\"Id\")[\"y\"].to_dict()\n",
    "    full_data.loc[:(split-1), \"y\"] = full_data.loc[:(split-1), \"id\"].map(y_id_dict)\n",
    "\n",
    "    train, test = full_data[:split], full_data[split:]\n",
    "    return (train, test)\n",
    "\n",
    "#columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    "#           + [\"solar_\" + str(i) for i in range(5)] + [\"y\"])\n",
    "#full_data = pd.read_csv(\"MODIS.csv\", header=None, names=columns)\n",
    "#split = 98000\n",
    "#train, test = full_data[:split].copy(), full_data[split:].copy()\n",
    "\n",
    "#train_copy, test_copy = load_data()\n",
    "\n",
    "train, test = load_data()\n",
    "\n",
    "# Parameters\n",
    "n_threads = -1\n",
    "random_seed = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reflectance_0</th>\n",
       "      <th>reflectance_1</th>\n",
       "      <th>reflectance_2</th>\n",
       "      <th>reflectance_3</th>\n",
       "      <th>reflectance_4</th>\n",
       "      <th>reflectance_5</th>\n",
       "      <th>reflectance_6</th>\n",
       "      <th>solar_0</th>\n",
       "      <th>solar_1</th>\n",
       "      <th>solar_2</th>\n",
       "      <th>solar_3</th>\n",
       "      <th>solar_4</th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.580642</td>\n",
       "      <td>2.482233</td>\n",
       "      <td>5.887092</td>\n",
       "      <td>4.732722</td>\n",
       "      <td>4.408482</td>\n",
       "      <td>3.830171</td>\n",
       "      <td>4.388508</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.338455</td>\n",
       "      <td>3.627796</td>\n",
       "      <td>4.723716</td>\n",
       "      <td>3.324726</td>\n",
       "      <td>2.743442</td>\n",
       "      <td>4.727652</td>\n",
       "      <td>2.810193</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.224569</td>\n",
       "      <td>3.522241</td>\n",
       "      <td>6.188831</td>\n",
       "      <td>4.389783</td>\n",
       "      <td>4.177616</td>\n",
       "      <td>4.945918</td>\n",
       "      <td>4.122848</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.717218</td>\n",
       "      <td>2.712012</td>\n",
       "      <td>5.024211</td>\n",
       "      <td>3.944907</td>\n",
       "      <td>3.393424</td>\n",
       "      <td>3.931973</td>\n",
       "      <td>3.489578</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.378857</td>\n",
       "      <td>3.644976</td>\n",
       "      <td>4.515292</td>\n",
       "      <td>3.223825</td>\n",
       "      <td>2.739952</td>\n",
       "      <td>4.599662</td>\n",
       "      <td>2.781574</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reflectance_0  reflectance_1  reflectance_2  reflectance_3  reflectance_4  \\\n",
       "0       1.580642       2.482233       5.887092       4.732722       4.408482   \n",
       "1       2.338455       3.627796       4.723716       3.324726       2.743442   \n",
       "2       2.224569       3.522241       6.188831       4.389783       4.177616   \n",
       "3       1.717218       2.712012       5.024211       3.944907       3.393424   \n",
       "4       2.378857       3.644976       4.515292       3.223825       2.739952   \n",
       "\n",
       "   reflectance_5  reflectance_6    solar_0   solar_1   solar_2   solar_3  \\\n",
       "0       3.830171       4.388508  22.572888  63.58724  88.05048  4.495216   \n",
       "1       4.727652       2.810193  22.572888  63.58724  88.05048  4.495216   \n",
       "2       4.945918       4.122848  22.572888  63.58724  88.05048  4.495216   \n",
       "3       3.931973       3.489578  22.572888  63.58724  88.05048  4.495216   \n",
       "4       4.599662       2.781574  22.572888  63.58724  88.05048  4.495216   \n",
       "\n",
       "     solar_4  id         y  \n",
       "0 -50.699904   1 -3.998082  \n",
       "1 -50.699904   1 -3.998082  \n",
       "2 -50.699904   1 -3.998082  \n",
       "3 -50.699904   1 -3.998082  \n",
       "4 -50.699904   1 -3.998082  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_excl = [\"id\", \"y\"]\n",
    "cols_orig = [c for c in train.columns if c not in cols_excl]\n",
    "\n",
    "# Remove outliers\n",
    "outliers_id = [21, 31, 72, 85, 135, 154, 165,\n",
    "               199, 232, 252, 255, 262, 289, 387,\n",
    "               393, 404, 408, 434, 488, 516, 578,\n",
    "               615, 617, 624, 633, 647, 683, 778,\n",
    "               785, 792, 817, 828, 917, 946, 960]\n",
    "\n",
    "#train = train[~train[\"id\"].isin(outliers_id)].reset_index(drop=True)\n",
    "\n",
    "# Standardise data for LR\n",
    "train[cols_orig] = scale(train[cols_orig])\n",
    "test[cols_orig] = scale(test[cols_orig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_orig]\n",
    "\n",
    "#regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                         # hidden_units=[9, 9],\n",
    "                                         #)#model_dir=\"./temp_log\")\n",
    "\n",
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()\n",
    "\n",
    "#def input_fn(data_set):\n",
    " #   feature_cols = {k: tf.constant(data_set[k].values) for k in cols_orig}\n",
    "  #  labels = tf.constant(data_set[\"y\"].values)\n",
    "    \n",
    "   # return feature_cols, labels\n",
    "\n",
    "#validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    " #   input_fn=lambda: input_fn(test),\n",
    "  #  early_stopping_rounds=100)\n",
    "\n",
    "#regressor.fit(input_fn=lambda: input_fn(train), steps=10)\n",
    "              #monitors=[validation_monitor])\n",
    "\n",
    "#ev = regressor.evaluate(input_fn=lambda: input_fn(train), steps=1)\n",
    "#loss_score = ev[\"loss\"]\n",
    "#print(\"Loss: {0:f}\".format(loss_score))\n",
    "\n",
    "#y = regressor.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to a list and print predictions\n",
    "#predictions = np.array(list(itertools.islice(y, 0, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pdf_weight(data):\n",
    "    cols = list(data.columns)\n",
    "    X = data.copy()\n",
    "    \n",
    "    y_mean_dict = X.groupby(\"id\")[\"y_hat\"].mean().to_dict()\n",
    "    y_std_dict = X.groupby(\"id\")[\"y_hat\"].std().to_dict()\n",
    "    X[\"y_hat_mean\"] = X[\"id\"].map(y_mean_dict)\n",
    "    X[\"y_hat_std\"] = X[\"id\"].map(y_std_dict)\n",
    "    X[\"pdf\"] = norm.pdf(X[\"y_hat\"], X[\"y_hat_mean\"], \n",
    "                        X[\"y_hat_std\"])\n",
    "    y_pdf_sum_dict = X.groupby(\"id\")[\"pdf\"].sum().to_dict()\n",
    "    X[\"pdf_sum\"] = X[\"id\"].map(y_pdf_sum_dict)\n",
    "    X[\"pdf\"] /= X[\"pdf_sum\"]\n",
    "    X[\"y_hat_weighted\"] = X[\"y_hat\"] * X[\"pdf\"]\n",
    "    \n",
    "    y_weighted_dict = X.groupby(\"id\")[\"y_hat_weighted\"].sum().to_dict()\n",
    "    X[\"y_hat_weighted_sum\"] = X[\"id\"].map(y_weighted_dict)\n",
    "    \n",
    "    return(X[cols + [\"y_hat_weighted_sum\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "models_weights = {\"dnn_1\": 1}#, \"dnn_2\": 0.2, \"dnn_3\": 0.2,\n",
    "                  #\"dnn_4\": 0.2, \"dnn_5\": 0.2}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "#models_cols = {\"dnn_1\": cols_dnn, \"dnn_2\": cols_dnn, \"dnn_3\": cols_dnn,\n",
    " #              \"dnn_4\": cols_dnn, \"dnn_5\": cols_dnn}\n",
    "    \n",
    "#learning_rate = 0.1\n",
    "\n",
    "# Scoring function in the hyperopt hyperparameters tuning.\n",
    "def scoring_function(parameters):\n",
    "    print(\"Training the model with parameters: \")\n",
    "    print(parameters)\n",
    "    average_RMSE = 0.0\n",
    "    n_splits = 5\n",
    "    \n",
    "    # Generate random integer for model_dir\n",
    "    random_int = np.random.randint(1000)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    nb_fold = 0\n",
    "    for train_index, validation_index in kf.split(train):\n",
    "        nb_fold += 1\n",
    "        train_fold, validation_fold = train.loc[train_index], train.loc[validation_index]\n",
    "        \n",
    "        # Remove outliers\n",
    "        #train_fold = train_fold[~train_fold[\"id\"].isin(outliers_id)].reset_index(drop=True)\n",
    "\n",
    "        feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "        model_dir = (\"./log_\"\n",
    "                     + str(parameters[\"steps\"]) + \"_\"\n",
    "                     + str(parameters[\"nb_neurons_1\"]) + \"_\"\n",
    "                     #+ str(parameters[\"nb_neurons_2\"])\n",
    "                     + str(nb_fold) + \"_\"\n",
    "                     + str(random_int)\n",
    "                    )\n",
    "        \n",
    "        # Tune number of layers\n",
    "        model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                  hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                #parameters[\"nb_neurons_2\"]],\n",
    "                                                  #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                   #   learning_rate=learning_rate,\n",
    "                                                    #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                  #dropout=parameters[\"dropout\"],\n",
    "                                                  model_dir=model_dir)\n",
    "\n",
    "        def input_fn(data_set):\n",
    "            feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "            labels = tf.constant(data_set[\"y\"].values)\n",
    "            return feature_cols, labels\n",
    "        \n",
    "        model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "        train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "        #for i, m in models.items():\n",
    "        temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "        # .predict() returns an iterator; convert to an array\n",
    "        y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "        train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "        # Use median value by id\n",
    "        y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "        RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train_fold[\"y\"]))\n",
    "        \n",
    "        # Prune outliers\n",
    "        RMSE_decreasing = True\n",
    "        count = 0\n",
    "        while (RMSE_decreasing):\n",
    "            count +=1\n",
    "            train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "            # Distance from the median for each bag\n",
    "            train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "            # Rank of each instance by bag\n",
    "            train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "            bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "            train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "            train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "            # Remove outliers\n",
    "            outliers_index = train_pred[\"rank\"] > (1 - parameters[\"outliers_threshold\"])\n",
    "            train_fold = train_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "\n",
    "            model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                      hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                    #parameters[\"nb_neurons_2\"]],\n",
    "                                                      #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                       #   learning_rate=learning_rate,\n",
    "                                                        #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                      #dropout=parameters[\"dropout\"],\n",
    "                                                      model_dir=model_dir)\n",
    "\n",
    "            model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "            # Compute new RMSE\n",
    "            train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            #for i, m in models.items():\n",
    "            temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "            y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "            new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train_fold[\"y\"]))\n",
    "            print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "            \n",
    "            if (abs(new_RMSE - RMSE) > parameters[\"gain_threshold\"]):\n",
    "            # 5 iterations of pruning\n",
    "            #if (count < 5):\n",
    "                RMSE = new_RMSE\n",
    "            else:\n",
    "                RMSE_decreasing = False\n",
    "        \n",
    "        # Bagging of RNN\n",
    "        # Bootstrap 1\n",
    "        train_fold_1 = train_fold\n",
    "        model_dnn_1 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                    hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                     #   learning_rate=learning_rate,\n",
    "                                                      #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                    #dropout=parameters[\"dropout\"])\n",
    "        model_dnn_1.fit(input_fn=lambda: input_fn(train_fold_1), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Boostrap 2\n",
    "        #train_fold_2 = resample(train_fold, random_state=random_seed).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_2 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_2.fit(input_fn=lambda: input_fn(train_fold_2), steps=parameters[\"steps\"])\n",
    "            \n",
    "        # Bootstrap 3\n",
    "        #train_fold_3 = resample(train_fold, random_state=(random_seed+1)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_3 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_3.fit(input_fn=lambda: input_fn(train_fold_3), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Bootstrap 4\n",
    "        #train_fold_4 = resample(train_fold, random_state=(random_seed+2)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_4 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_4.fit(input_fn=lambda: input_fn(train_fold_4), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Bootstrap 5\n",
    "        #train_fold_5 = resample(train_fold, random_state=(random_seed+3)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_5 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_5.fit(input_fn=lambda: input_fn(train_fold_5), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Changed to model_dnn instead of model_dnn_1\n",
    "        models = {\"dnn_1\": model_dnn_1}#, \"dnn_2\": model_dnn_2, \"dnn_3\": model_dnn_3,\n",
    "                  #\"dnn_4\": model_dnn_4, \"dnn_5\": model_dnn_5}\n",
    "        \n",
    "        # Compute RMSE on validation set\n",
    "        validation_pred = validation_fold[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "        for i, m in models.items():\n",
    "            temp = m.predict(input_fn=lambda: input_fn(validation_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            validation_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "            \n",
    "        # Weight each instance by gaussian pdf\n",
    "        #validation_pred = pdf_weight(validation_pred)\n",
    "        # Use median value by id\n",
    "        y_hat_med = validation_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "        \n",
    "        #validation_pred[\"y_hat_weighted_sum\"]\n",
    "        #validation_fold[\"id\"].map(y_hat_med).values\n",
    "        RMSE = np.sqrt(mean_squared_error(validation_fold[\"id\"].map(y_hat_med).values, validation_fold[\"y\"]))\n",
    "        average_RMSE += RMSE\n",
    "        print(\"Validation fold {0} RMSE: {1}\".format(nb_fold, RMSE))\n",
    "\n",
    "    average_RMSE /= n_splits\n",
    "\n",
    "    print(\"Cross-validation score: {0}\\n\".format(average_RMSE))\n",
    "    \n",
    "    return {\"loss\": average_RMSE, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.02, 'steps': 1500, 'nb_neurons_1': 8, 'outliers_threshold': 0.05}\n",
      "Pruning 1 RMSE: 0.757836956217\n",
      "Pruning 2 RMSE: 0.729928709386\n",
      "Pruning 3 RMSE: 0.71654657983\n",
      "Validation fold 1 RMSE: 0.667934138656\n",
      "Pruning 1 RMSE: 0.682366766358\n",
      "Pruning 2 RMSE: 0.668189360715\n",
      "Validation fold 2 RMSE: 0.763847560487\n",
      "Pruning 1 RMSE: 0.675626676576\n",
      "Pruning 2 RMSE: 0.650075468901\n",
      "Pruning 3 RMSE: 0.640139636178\n",
      "Validation fold 3 RMSE: 0.726100423631\n",
      "Pruning 1 RMSE: 0.694236026879\n",
      "Pruning 2 RMSE: 0.681407401412\n",
      "Validation fold 4 RMSE: 0.711927144269\n",
      "Pruning 1 RMSE: 0.660835796781\n",
      "Pruning 2 RMSE: 0.648427726779\n",
      "Validation fold 5 RMSE: 0.776227210926\n",
      "Cross-validation score: 0.729207295594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Grid to pick parameters from.\n",
    "parameters_grid = {\"steps\"             : hp.choice(\"steps\", np.arange(1500, 2000, 100, dtype=int)),\n",
    "                   \"nb_neurons_1\"      : hp.choice(\"nb_neurons_1\", np.arange(7, 13, 1, dtype=int)),\n",
    "                   \"outliers_threshold\": hp.quniform(\"outliers_threshold\", 0.05, 0.051, 0.01),\n",
    "                   \"gain_threshold\"    : hp.quniform(\"gain_threshold\", 0.01, 0.02, 0.005)\n",
    "                   #\"dropout\": hp.quniform(\"dropout\", 0.2, 0.4, 0.1)\n",
    "                   #\"l2_reg\": hp.quniform(\"l2_reg\", 0.00, 0.005, 0.01)\n",
    "                   #\"nb_neurons_2\": hp.choice(\"nb_neurons_2\", np.arange(5, 10, 1, dtype=int))\n",
    "                  }\n",
    "# Record the information about the cross-validation.\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(scoring_function, parameters_grid, algo=tpe.suggest, max_evals=1, \n",
    "            trials=trials)\n",
    "\n",
    "computing_time = time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "1 DNN\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 900, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.720018452178\n",
    "Pruning 2 RMSE: 0.694637808863\n",
    "Pruning 3 RMSE: 0.682604548885\n",
    "Pruning 4 RMSE: 0.675240927353\n",
    "Validation fold 1 RMSE: 0.657330001629\n",
    "Pruning 1 RMSE: 0.67466681128\n",
    "Pruning 2 RMSE: 0.65951277037\n",
    "Pruning 3 RMSE: 0.649785148888\n",
    "Validation fold 2 RMSE: 0.748604123648\n",
    "Pruning 1 RMSE: 0.68920733019\n",
    "Pruning 2 RMSE: 0.673928761686\n",
    "Pruning 3 RMSE: 0.663794898173\n",
    "Pruning 4 RMSE: 0.656852425765\n",
    "Validation fold 3 RMSE: 0.75238232566\n",
    "Pruning 1 RMSE: 0.695945822399\n",
    "Pruning 2 RMSE: 0.677436347483\n",
    "Pruning 3 RMSE: 0.667360192164\n",
    "Pruning 4 RMSE: 0.661270048569\n",
    "Validation fold 4 RMSE: 0.703611985649\n",
    "Pruning 1 RMSE: 0.675491954494\n",
    "Pruning 2 RMSE: 0.654833366872\n",
    "Pruning 3 RMSE: 0.641692836958\n",
    "Pruning 4 RMSE: 0.633366415513\n",
    "Validation fold 5 RMSE: 0.784596896669\n",
    "Cross-validation score: 0.729305066651\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "5 DNN and use the pruning DNN\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.674073882641\n",
    "Pruning 2 RMSE: 0.662346176042\n",
    "Validation fold 1 RMSE: 0.641617979853\n",
    "Pruning 1 RMSE: 0.635676000166\n",
    "Pruning 2 RMSE: 0.619615432596\n",
    "Pruning 3 RMSE: 0.610423043833\n",
    "Validation fold 2 RMSE: 0.73246715849\n",
    "Pruning 1 RMSE: 0.643307566169\n",
    "Pruning 2 RMSE: 0.62938122591\n",
    "Validation fold 3 RMSE: 0.710291994342\n",
    "Pruning 1 RMSE: 0.659927208941\n",
    "Pruning 2 RMSE: 0.647666579446\n",
    "Validation fold 4 RMSE: 0.670328953596\n",
    "Pruning 1 RMSE: 0.632532403066\n",
    "Pruning 2 RMSE: 0.614925033513\n",
    "Pruning 3 RMSE: 0.605334750968\n",
    "Validation fold 5 RMSE: 0.753588096326\n",
    "Cross-validation score: 0.701658836521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1 DNN with model_dir corrected\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.005, 'steps': 1000, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.708254362329\n",
    "Pruning 2 RMSE: 0.690809364036\n",
    "Pruning 3 RMSE: 0.681259286244\n",
    "Pruning 4 RMSE: 0.675679762642\n",
    "Pruning 5 RMSE: 0.670965511179\n",
    "Validation fold 1 RMSE: 0.64861798817\n",
    "Pruning 1 RMSE: 0.658292571898\n",
    "Pruning 2 RMSE: 0.645737414652\n",
    "Pruning 3 RMSE: 0.63797501753\n",
    "Pruning 4 RMSE: 0.632583378549\n",
    "Pruning 5 RMSE: 0.627584029004\n",
    "Validation fold 2 RMSE: 0.723643911448\n",
    "Pruning 1 RMSE: 0.687961545764\n",
    "Pruning 2 RMSE: 0.672401033846\n",
    "Pruning 3 RMSE: 0.660611288466\n",
    "Pruning 4 RMSE: 0.650237174155\n",
    "Pruning 5 RMSE: 0.64198570573\n",
    "Pruning 6 RMSE: 0.636986731147\n",
    "Validation fold 3 RMSE: 0.719981056486\n",
    "Pruning 1 RMSE: 0.697781938911\n",
    "Pruning 2 RMSE: 0.681268268398\n",
    "Pruning 3 RMSE: 0.671483140797\n",
    "Pruning 4 RMSE: 0.666088232133\n",
    "Pruning 5 RMSE: 0.661858777702\n",
    "Validation fold 4 RMSE: 0.715854578144\n",
    "Pruning 1 RMSE: 0.66224449426\n",
    "Pruning 2 RMSE: 0.644538081642\n",
    "Pruning 3 RMSE: 0.634079022739\n",
    "Pruning 4 RMSE: 0.624630852786\n",
    "Pruning 5 RMSE: 0.619603795183\n",
    "Pruning 6 RMSE: 0.61598970139\n",
    "Validation fold 5 RMSE: 0.762927716346\n",
    "Cross-validation score: 0.714205050119\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 1100, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.713960264442\n",
    "Pruning 2 RMSE: 0.701637487701\n",
    "Pruning 3 RMSE: 0.69375349128\n",
    "Validation fold 1 RMSE: 0.641879797964\n",
    "Pruning 1 RMSE: 0.671532366166\n",
    "Pruning 2 RMSE: 0.656442799709\n",
    "Pruning 3 RMSE: 0.645330625403\n",
    "Pruning 4 RMSE: 0.638014896705\n",
    "Validation fold 2 RMSE: 0.741045867638\n",
    "Pruning 1 RMSE: 0.690110283558\n",
    "Pruning 2 RMSE: 0.678068607072\n",
    "Pruning 3 RMSE: 0.662923307388\n",
    "Pruning 4 RMSE: 0.654881538105\n",
    "Validation fold 3 RMSE: 0.687377107282\n",
    "Pruning 1 RMSE: 0.70616839345\n",
    "Pruning 2 RMSE: 0.692895897509\n",
    "Pruning 3 RMSE: 0.686021953655\n",
    "Validation fold 4 RMSE: 0.688327537634\n",
    "Pruning 1 RMSE: 0.66943420746\n",
    "Pruning 2 RMSE: 0.65286815522\n",
    "Pruning 3 RMSE: 0.638691155949\n",
    "Pruning 4 RMSE: 0.627600556022\n",
    "Pruning 5 RMSE: 0.622316478\n",
    "Validation fold 5 RMSE: 0.765910014041\n",
    "Cross-validation score: 0.704908064912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6523287945244072"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(trials.losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gain_threshold</th>\n",
       "      <th>nb_neurons_1</th>\n",
       "      <th>outliers_threshold</th>\n",
       "      <th>steps</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0075</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>900</td>\n",
       "      <td>0.652329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gain_threshold  nb_neurons_1  outliers_threshold  steps     score\n",
       "0          0.0075            10                0.05    900  0.652329"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best parameters as a csv.\n",
    "best_parameters = pd.DataFrame({key: [value] for (key, value) in \n",
    "                                zip(space_eval(parameters_grid, best).keys(),\n",
    "                                    space_eval(parameters_grid, best).values())})\n",
    "# Add the corresponding score.\n",
    "best_parameters[\"score\"] = min(trials.losses())\n",
    "best_parameters.to_csv(\"best_parameters_7.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 1 RMSE: 0.623854414971\n",
      "Pruning 2 RMSE: 0.612112933147\n",
      "Pruning 3 RMSE: 0.605667733108\n"
     ]
    }
   ],
   "source": [
    "cols_dnn = cols_orig\n",
    "models_weights = {\"dnn_1\": 1.0}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "model_dir = \"./log_submit_6\"\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "# Tune number of layers\n",
    "model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                          hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                          model_dir=model_dir)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "        \n",
    "train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to an array\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train[\"y\"]))\n",
    "        \n",
    "# Prune outliers\n",
    "RMSE_decreasing = True\n",
    "count = 0\n",
    "while (RMSE_decreasing):\n",
    "    count += 1\n",
    "    train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "    # Distance from the median for each bag\n",
    "    train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "    # Rank of each instance by bag\n",
    "    train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "    bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "    train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "    train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    outliers_index = train_pred[\"rank\"] > (1 - best_parameters[\"outliers_threshold\"][0])\n",
    "    train = train.loc[~outliers_index, :].reset_index(drop=True)\n",
    "\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "    # Use median value by id\n",
    "    y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "    new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train[\"y\"]))\n",
    "    print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "\n",
    "    if (abs(new_RMSE - RMSE) > best_parameters[\"gain_threshold\"][0]):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        RMSE_decreasing = False\n",
    "        \n",
    "# Training model\n",
    "model_dnn_1 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_1\"][0]])\n",
    "model_dnn_1.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "\n",
    "models = {\"dnn_1\": model_dnn_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pred = test[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "for i, m in models.items():\n",
    "    temp = m.predict(input_fn=lambda: input_fn(test))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    test_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = test_pred.groupby(\"id\").median()[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71423164615804291"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(test_pred[\"id\"].map(y_hat_med).values, test[\"y\"]))\n",
    "RMSE\n",
    "#0.65725435012348465 for Pred 4\n",
    "#0.65362864377856866 for Pred 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kaggle_pred = pd.DataFrame({\"Id\": y_hat_med.index, \"y\": y_hat_med.values})\n",
    "kaggle_pred.to_csv(\"Prediction_6.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark:\n",
    "* Submit 1 (ensemble of xgboost + 2 ridge with instances model)\n",
    "eta\teval_metric\tgamma\tlambda\tmax_depth\tmin_child_weight\tnthread\tobjective\tseed\tsilent\tsubsample\tscore\n",
    "0.91834 Public LB 300 trees 0.09\trmse\t0.2\t0.8\t4\t4.0\t-1\treg:linear\t22\t0\t0.7\t0.883339 (cross-val)\n",
    "    \n",
    "* Submit 2\n",
    "\n",
    "Pruning with linear regression\n",
    "then add contributions of aggregated xgboost + linear model\n",
    "\n",
    "0.78181 Public LB  0.779345 CV\n",
    "\n",
    "* Submit 3\n",
    "DNN pruning\n",
    "(wrong CV)\n",
    "0.73270 Public LB 0.663128 CV with DNN 10 neurons 900 steps gain_threshold = 0.01 outliers_threshold = 0.05\n",
    "\n",
    "* Prediction 4\n",
    "LB 0.74713\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.701658836521\n",
    "using fist pruning DNN\n",
    "* Prediction 5\n",
    "LB 0.74453\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.01, 'steps': 2400, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.707300896236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Dropout\n",
    "L2 regu\n",
    "Validation set instead of cross val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gamma_i_j(j, pi_i, X_i, y_i, delta):\n",
    "    out = 0.0\n",
    "    out = pi_i[j] * norm.pdf(y_i, X_i[j], delta)\n",
    "    out /= sum(pi_i * norm.pdf(y_i, X_i, delta))\n",
    "    return(out)\n",
    "\n",
    "def EM_Q(pi, X, y, delta):\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "cols_dnn = cols_orig\n",
    "\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "\n",
    "model_dir_f = \"./log_submit_6\"\n",
    "\n",
    "# Fit DNN regressor\n",
    "model_dnn_f = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                            model_dir=model_dir_f)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn_f.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "# Train prediction\n",
    "train_pred = train[[\"id\"]].assign(y_hat=0, pi_hat=0)\n",
    "temp = model_dnn_f.predict(input_fn=lambda: input_fn(train))\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Fit DNN softmax\n",
    "model_dir_g = \"./g_log_EM\"\n",
    "model_dnn_g = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_g\"][0]],\n",
    "                                            model_dir=model_dir_g)\n",
    "\n",
    "model_dnn_g.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_g\"][0])\n",
    "temp = model_dnn_g.predict(input_fn=lambda: input_fn(train))\n",
    "pi_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "\n",
    "# Compute softmax\n",
    "train_pred[\"pi_hat\"] = np.exp(pi_hat)\n",
    "pi_hat_sum_dict = train_pred.groupby(\"id\")[\"pi_hat\"].sum().to_dict()\n",
    "# \"map\" is actually much faster than \"replace\"\n",
    "train_pred[\"pi_hat_sum\"] = train_pred[\"id\"].map(pi_hat_sum_dict)\n",
    "train_pred[\"pi_hat\"] /= train_pred[\"pi_hat_sum\"]\n",
    "\n",
    "# EM algorithm\n",
    "# Change to Q\n",
    "EM_decreasing = True\n",
    "nb_iteration = 0\n",
    "while (EM_decreasing):\n",
    "    nb_iteration +=1\n",
    "\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "\n",
    "    print(\"Iteration {0} EM: {1}\".format(nb_iteration, new_RMSE))\n",
    "\n",
    "    if (abs(new_RMSE - RMSE) > 0.1):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        EM_decreasing = False\n",
    "\n",
    "models = {\"dnn_1\": model_dnn}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
