{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "from hyperopt import STATUS_OK, hp, fmin, tpe, Trials, space_eval\n",
    "\n",
    "from time import time\n",
    "import operator\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "def load_data():\n",
    "    full_data = pd.read_csv(\"X.csv\")\n",
    "    train_y = pd.read_csv(\"ytr.csv\")\n",
    "    # Rename columns to something more interpretable\n",
    "    columns = ([\"reflectance_\" + str(i) for i in range(7)]\n",
    "               + [\"solar_\" + str(i) for i in range(5)] + [\"id\"])\n",
    "    full_data.columns = columns\n",
    "    # Add y to the data frame\n",
    "    split = 98000\n",
    "    y_id_dict = train_y.set_index(\"Id\")[\"y\"].to_dict()\n",
    "    full_data.loc[:(split-1), \"y\"] = full_data.loc[:(split-1), \"id\"].map(y_id_dict)\n",
    "\n",
    "    train, test = full_data[:split], full_data[split:]\n",
    "    return (train, test)\n",
    "\n",
    "columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    "           + [\"solar_\" + str(i) for i in range(5)] + [\"y\"])\n",
    "full_data = pd.read_csv(\"MODIS.csv\", header=None, names=columns)\n",
    "split = 98000\n",
    "train, test = full_data[:split].copy(), full_data[split:].copy()\n",
    "\n",
    "#train_copy, test_copy = load_data()\n",
    "\n",
    "#train, test = load_data()\n",
    "\n",
    "# Parameters\n",
    "n_threads = -1\n",
    "random_seed = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reflectance_0</th>\n",
       "      <th>reflectance_1</th>\n",
       "      <th>reflectance_2</th>\n",
       "      <th>reflectance_3</th>\n",
       "      <th>reflectance_4</th>\n",
       "      <th>reflectance_5</th>\n",
       "      <th>reflectance_6</th>\n",
       "      <th>solar_0</th>\n",
       "      <th>solar_1</th>\n",
       "      <th>solar_2</th>\n",
       "      <th>solar_3</th>\n",
       "      <th>solar_4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.088535</td>\n",
       "      <td>0.050097</td>\n",
       "      <td>0.007748</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.052130</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.150211</td>\n",
       "      <td>0.091345</td>\n",
       "      <td>0.062856</td>\n",
       "      <td>0.140568</td>\n",
       "      <td>0.076832</td>\n",
       "      <td>0.032414</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.041447</td>\n",
       "      <td>0.276798</td>\n",
       "      <td>0.089301</td>\n",
       "      <td>0.072769</td>\n",
       "      <td>0.237950</td>\n",
       "      <td>0.109721</td>\n",
       "      <td>0.036960</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.029073</td>\n",
       "      <td>0.027024</td>\n",
       "      <td>0.088950</td>\n",
       "      <td>0.052317</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>28.420588</td>\n",
       "      <td>146.782941</td>\n",
       "      <td>20.686471</td>\n",
       "      <td>100.594706</td>\n",
       "      <td>159.884706</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  reflectance_0  reflectance_1  reflectance_2  reflectance_3  \\\n",
       "0   1       0.026993       0.012067       0.088535       0.050097   \n",
       "1   1       0.029457       0.019613       0.087705       0.052130   \n",
       "2   1       0.038491       0.150211       0.091345       0.062856   \n",
       "3   1       0.041447       0.276798       0.089301       0.072769   \n",
       "4   1       0.029073       0.027024       0.088950       0.052317   \n",
       "\n",
       "   reflectance_4  reflectance_5  reflectance_6    solar_0     solar_1  \\\n",
       "0       0.007748       0.004051       0.002929  28.420588  146.782941   \n",
       "1       0.016930       0.010574       0.003654  28.420588  146.782941   \n",
       "2       0.140568       0.076832       0.032414  28.420588  146.782941   \n",
       "3       0.237950       0.109721       0.036960  28.420588  146.782941   \n",
       "4       0.021162       0.011535       0.005997  28.420588  146.782941   \n",
       "\n",
       "     solar_2     solar_3     solar_4         y  \n",
       "0  20.686471  100.594706  159.884706  0.075627  \n",
       "1  20.686471  100.594706  159.884706  0.075627  \n",
       "2  20.686471  100.594706  159.884706  0.075627  \n",
       "3  20.686471  100.594706  159.884706  0.075627  \n",
       "4  20.686471  100.594706  159.884706  0.075627  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_excl = [\"id\", \"y\"]\n",
    "cols_orig = [c for c in train.columns if c not in cols_excl]\n",
    "\n",
    "# Standardise data for LR\n",
    "train[cols_orig] = scale(train[cols_orig])\n",
    "test[cols_orig] = scale(test[cols_orig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_orig]\n",
    "\n",
    "#regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                         # hidden_units=[9, 9],\n",
    "                                         #)#model_dir=\"./temp_log\")\n",
    "\n",
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()\n",
    "\n",
    "#def input_fn(data_set):\n",
    " #   feature_cols = {k: tf.constant(data_set[k].values) for k in cols_orig}\n",
    "  #  labels = tf.constant(data_set[\"y\"].values)\n",
    "    \n",
    "   # return feature_cols, labels\n",
    "\n",
    "#validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    " #   input_fn=lambda: input_fn(test),\n",
    "  #  early_stopping_rounds=100)\n",
    "\n",
    "#regressor.fit(input_fn=lambda: input_fn(train), steps=10)\n",
    "              #monitors=[validation_monitor])\n",
    "\n",
    "#ev = regressor.evaluate(input_fn=lambda: input_fn(train), steps=1)\n",
    "#loss_score = ev[\"loss\"]\n",
    "#print(\"Loss: {0:f}\".format(loss_score))\n",
    "\n",
    "#y = regressor.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to a list and print predictions\n",
    "#predictions = np.array(list(itertools.islice(y, 0, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "models_weights = {\"dnn_1\": 1}#, \"dnn_2\": 0.2, \"dnn_3\": 0.2,\n",
    "                  #\"dnn_4\": 0.2, \"dnn_5\": 0.2}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "#models_cols = {\"dnn_1\": cols_dnn, \"dnn_2\": cols_dnn, \"dnn_3\": cols_dnn,\n",
    " #              \"dnn_4\": cols_dnn, \"dnn_5\": cols_dnn}\n",
    "    \n",
    "learning_rate = 0.1\n",
    "\n",
    "# Scoring function in the hyperopt hyperparameters tuning.\n",
    "def scoring_function(parameters):\n",
    "    print(\"Training the model with parameters: \")\n",
    "    print(parameters)\n",
    "    average_RMSE = 0.0\n",
    "    n_splits = 5\n",
    "    \n",
    "    # Generate random integer for model_dir\n",
    "    random_int = np.random.randint(1000)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    nb_fold = 0\n",
    "    for train_index, validation_index in kf.split(train):\n",
    "        nb_fold += 1\n",
    "        train_fold, validation_fold = train.loc[train_index], train.loc[validation_index] \n",
    "\n",
    "        feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "        model_dir = (\"./log_\"\n",
    "                     + str(parameters[\"steps\"]) + \"_\"\n",
    "                     + str(parameters[\"nb_neurons_1\"]) + \"_\"\n",
    "                     #+ str(parameters[\"nb_neurons_2\"])\n",
    "                     + str(nb_fold) + \"_\"\n",
    "                     + str(random_int)\n",
    "                    )\n",
    "        \n",
    "        # Tune number of layers\n",
    "        model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                  hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                #parameters[\"nb_neurons_2\"]],\n",
    "                                                  optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                      learning_rate=learning_rate,\n",
    "                                                      l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                  dropout=parameters[\"dropout\"],\n",
    "                                                  model_dir=model_dir)\n",
    "\n",
    "        def input_fn(data_set):\n",
    "            feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "            labels = tf.constant(data_set[\"y\"].values)\n",
    "            return feature_cols, labels\n",
    "        \n",
    "        model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "        train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "        #for i, m in models.items():\n",
    "        temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "        # .predict() returns an iterator; convert to an array\n",
    "        y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "        train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "        # Use median value by id\n",
    "        y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "        RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train_fold[\"y\"]))\n",
    "        \n",
    "        # Prune outliers\n",
    "        RMSE_decreasing = True\n",
    "        count = 0\n",
    "        while (RMSE_decreasing):\n",
    "            count +=1\n",
    "            train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "            # Distance from the median for each bag\n",
    "            train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "            # Rank of each instance by bag\n",
    "            train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "            bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "            train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "            train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "            # Remove outliers\n",
    "            outliers_index = train_pred[\"rank\"] > (1 - parameters[\"outliers_threshold\"])\n",
    "            train_fold = train_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "\n",
    "            model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                      hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                    #parameters[\"nb_neurons_2\"]],\n",
    "                                                      optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                          learning_rate=learning_rate,\n",
    "                                                          l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                      dropout=parameters[\"dropout\"],\n",
    "                                                      model_dir=model_dir)\n",
    "\n",
    "            model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "            # Compute new RMSE\n",
    "            train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            #for i, m in models.items():\n",
    "            temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "            y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "            new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train_fold[\"y\"]))\n",
    "            print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "                \n",
    "            # 5 iterations of pruning\n",
    "            #if (abs(new_RMSE - RMSE) > parameters[\"gain_threshold\"]):\n",
    "            if (count < 5):\n",
    "                RMSE = new_RMSE\n",
    "            else:\n",
    "                RMSE_decreasing = False\n",
    "        \n",
    "        # Bagging of RNN\n",
    "        # Bootstrap 1\n",
    "        train_fold_1 = train_fold\n",
    "        model_dnn_1 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                    hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                    optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                        learning_rate=learning_rate,\n",
    "                                                        l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                    dropout=parameters[\"dropout\"])\n",
    "        model_dnn_1.fit(input_fn=lambda: input_fn(train_fold_1), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Boostrap 2\n",
    "        #train_fold_2 = resample(train_fold, random_state=random_seed).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_2 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_2.fit(input_fn=lambda: input_fn(train_fold_2), steps=parameters[\"steps\"])\n",
    "            \n",
    "        # Bootstrap 3\n",
    "        #train_fold_3 = resample(train_fold, random_state=(random_seed+1)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_3 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_3.fit(input_fn=lambda: input_fn(train_fold_3), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Bootstrap 4\n",
    "        #train_fold_4 = resample(train_fold, random_state=(random_seed+2)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_4 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_4.fit(input_fn=lambda: input_fn(train_fold_4), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Bootstrap 5\n",
    "        #train_fold_5 = resample(train_fold, random_state=(random_seed+3)).sort_values(by=[\"id\"]).reset_index(drop=True)\n",
    "        #model_dnn_5 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "         #                                           hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        #model_dnn_5.fit(input_fn=lambda: input_fn(train_fold_5), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Changed to model_dnn instead of model_dnn_1\n",
    "        models = {\"dnn_1\": model_dnn_1}#, \"dnn_2\": model_dnn_2, \"dnn_3\": model_dnn_3,\n",
    "                  #\"dnn_4\": model_dnn_4, \"dnn_5\": model_dnn_5}\n",
    "        \n",
    "        # Compute RMSE on validation set\n",
    "        validation_pred = validation_fold[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "        for i, m in models.items():\n",
    "            temp = m.predict(input_fn=lambda: input_fn(validation_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            validation_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "            \n",
    "        # Use median value by id\n",
    "        y_hat_med = validation_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "        \n",
    "        RMSE = np.sqrt(mean_squared_error(validation_pred[\"id\"].map(y_hat_med).values, validation_fold[\"y\"]))\n",
    "        average_RMSE += RMSE\n",
    "        print(\"Validation fold {0} RMSE: {1}\".format(nb_fold, RMSE))\n",
    "\n",
    "    average_RMSE /= n_splits\n",
    "\n",
    "    print(\"Cross-validation score: {0}\\n\".format(average_RMSE))\n",
    "    \n",
    "    return {\"loss\": average_RMSE, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 800, 'dropout': 0.0, 'l2_reg': 0.0, 'outliers_threshold': 0.05, 'nb_neurons_1': 10}\n",
      "Pruning 1 RMSE: 0.101383996718\n",
      "Pruning 2 RMSE: 0.0978095948641\n",
      "Pruning 3 RMSE: 0.0953234314593\n",
      "Pruning 4 RMSE: 0.0935930969832\n",
      "Pruning 5 RMSE: 0.0917166903942\n",
      "Validation fold 1 RMSE: 0.103266539533\n",
      "Pruning 1 RMSE: 0.103283954457\n",
      "Pruning 2 RMSE: 0.0978579181647\n",
      "Pruning 3 RMSE: 0.0943521111809\n",
      "Pruning 4 RMSE: 0.0918541919196\n",
      "Pruning 5 RMSE: 0.0899483757189\n",
      "Validation fold 2 RMSE: 0.126386504768\n",
      "Pruning 1 RMSE: 0.106117454357\n",
      "Pruning 2 RMSE: 0.098471412368\n",
      "Pruning 3 RMSE: 0.0941309863349\n",
      "Pruning 4 RMSE: 0.0913804097672\n",
      "Pruning 5 RMSE: 0.0895612757944\n",
      "Validation fold 3 RMSE: 0.124222866517\n",
      "Pruning 1 RMSE: 0.0995391028902\n",
      "Pruning 2 RMSE: 0.0951715062324\n",
      "Pruning 3 RMSE: 0.0921694052346\n",
      "Pruning 4 RMSE: 0.0903328714767\n",
      "Pruning 5 RMSE: 0.0891820281494\n",
      "Validation fold 4 RMSE: 0.109456731591\n",
      "Pruning 1 RMSE: 0.0952387198159\n",
      "Pruning 2 RMSE: 0.0915773097614\n",
      "Pruning 3 RMSE: 0.0888468806946\n",
      "Pruning 4 RMSE: 0.0867590450118\n",
      "Pruning 5 RMSE: 0.0851775296271\n",
      "Validation fold 5 RMSE: 0.173478892574\n",
      "Cross-validation score: 0.127362306996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Grid to pick parameters from.\n",
    "parameters_grid = {\"steps\"             : hp.choice(\"steps\", np.arange(600, 1000, 100, dtype=int)),\n",
    "                   \"nb_neurons_1\"      : hp.choice(\"nb_neurons_1\", np.arange(9, 12, 1, dtype=int)),\n",
    "                   \"outliers_threshold\": hp.quniform(\"outliers_threshold\", 0.05, 0.051, 0.01),\n",
    "                   \"gain_threshold\"    : hp.quniform(\"gain_threshold\", 0.005, 0.02, 0.005),\n",
    "                   \"dropout\": hp.quniform(\"dropout\", 0.0, 0.015, 0.1),\n",
    "                   \"l2_reg\": hp.quniform(\"l2_reg\", 0.00, 0.005, 0.01)\n",
    "                   #\"nb_neurons_2\": hp.choice(\"nb_neurons_2\", np.arange(5, 10, 1, dtype=int))\n",
    "                  }\n",
    "# Record the information about the cross-validation.\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(scoring_function, parameters_grid, algo=tpe.suggest, max_evals=1, \n",
    "            trials=trials)\n",
    "\n",
    "computing_time = time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "5 DNN and use the pruning DNN\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.674073882641\n",
    "Pruning 2 RMSE: 0.662346176042\n",
    "Validation fold 1 RMSE: 0.641617979853\n",
    "Pruning 1 RMSE: 0.635676000166\n",
    "Pruning 2 RMSE: 0.619615432596\n",
    "Pruning 3 RMSE: 0.610423043833\n",
    "Validation fold 2 RMSE: 0.73246715849\n",
    "Pruning 1 RMSE: 0.643307566169\n",
    "Pruning 2 RMSE: 0.62938122591\n",
    "Validation fold 3 RMSE: 0.710291994342\n",
    "Pruning 1 RMSE: 0.659927208941\n",
    "Pruning 2 RMSE: 0.647666579446\n",
    "Validation fold 4 RMSE: 0.670328953596\n",
    "Pruning 1 RMSE: 0.632532403066\n",
    "Pruning 2 RMSE: 0.614925033513\n",
    "Pruning 3 RMSE: 0.605334750968\n",
    "Validation fold 5 RMSE: 0.753588096326\n",
    "Cross-validation score: 0.701658836521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1 DNN with model_dir corrected\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.005, 'steps': 1000, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.708254362329\n",
    "Pruning 2 RMSE: 0.690809364036\n",
    "Pruning 3 RMSE: 0.681259286244\n",
    "Pruning 4 RMSE: 0.675679762642\n",
    "Pruning 5 RMSE: 0.670965511179\n",
    "Validation fold 1 RMSE: 0.64861798817\n",
    "Pruning 1 RMSE: 0.658292571898\n",
    "Pruning 2 RMSE: 0.645737414652\n",
    "Pruning 3 RMSE: 0.63797501753\n",
    "Pruning 4 RMSE: 0.632583378549\n",
    "Pruning 5 RMSE: 0.627584029004\n",
    "Validation fold 2 RMSE: 0.723643911448\n",
    "Pruning 1 RMSE: 0.687961545764\n",
    "Pruning 2 RMSE: 0.672401033846\n",
    "Pruning 3 RMSE: 0.660611288466\n",
    "Pruning 4 RMSE: 0.650237174155\n",
    "Pruning 5 RMSE: 0.64198570573\n",
    "Pruning 6 RMSE: 0.636986731147\n",
    "Validation fold 3 RMSE: 0.719981056486\n",
    "Pruning 1 RMSE: 0.697781938911\n",
    "Pruning 2 RMSE: 0.681268268398\n",
    "Pruning 3 RMSE: 0.671483140797\n",
    "Pruning 4 RMSE: 0.666088232133\n",
    "Pruning 5 RMSE: 0.661858777702\n",
    "Validation fold 4 RMSE: 0.715854578144\n",
    "Pruning 1 RMSE: 0.66224449426\n",
    "Pruning 2 RMSE: 0.644538081642\n",
    "Pruning 3 RMSE: 0.634079022739\n",
    "Pruning 4 RMSE: 0.624630852786\n",
    "Pruning 5 RMSE: 0.619603795183\n",
    "Pruning 6 RMSE: 0.61598970139\n",
    "Validation fold 5 RMSE: 0.762927716346\n",
    "Cross-validation score: 0.714205050119\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 1100, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.713960264442\n",
    "Pruning 2 RMSE: 0.701637487701\n",
    "Pruning 3 RMSE: 0.69375349128\n",
    "Validation fold 1 RMSE: 0.641879797964\n",
    "Pruning 1 RMSE: 0.671532366166\n",
    "Pruning 2 RMSE: 0.656442799709\n",
    "Pruning 3 RMSE: 0.645330625403\n",
    "Pruning 4 RMSE: 0.638014896705\n",
    "Validation fold 2 RMSE: 0.741045867638\n",
    "Pruning 1 RMSE: 0.690110283558\n",
    "Pruning 2 RMSE: 0.678068607072\n",
    "Pruning 3 RMSE: 0.662923307388\n",
    "Pruning 4 RMSE: 0.654881538105\n",
    "Validation fold 3 RMSE: 0.687377107282\n",
    "Pruning 1 RMSE: 0.70616839345\n",
    "Pruning 2 RMSE: 0.692895897509\n",
    "Pruning 3 RMSE: 0.686021953655\n",
    "Validation fold 4 RMSE: 0.688327537634\n",
    "Pruning 1 RMSE: 0.66943420746\n",
    "Pruning 2 RMSE: 0.65286815522\n",
    "Pruning 3 RMSE: 0.638691155949\n",
    "Pruning 4 RMSE: 0.627600556022\n",
    "Pruning 5 RMSE: 0.622316478\n",
    "Validation fold 5 RMSE: 0.765910014041\n",
    "Cross-validation score: 0.704908064912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min(trials.losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save the best parameters as a csv.\n",
    "best_parameters = pd.DataFrame({key: [value] for (key, value) in \n",
    "                                zip(space_eval(parameters_grid, best).keys(),\n",
    "                                    space_eval(parameters_grid, best).values())})\n",
    "# Add the corresponding score.\n",
    "best_parameters[\"score\"] = min(trials.losses())\n",
    "best_parameters.to_csv(\"best_parameters_6.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gamma_i_j(j, pi_i, X_i, y_i, delta):\n",
    "    out = 0.0\n",
    "    out = pi_i[j] * norm.pdf(y_i, X_i[j], delta)\n",
    "    out /= sum(pi_i * norm.pdf(y_i, X_i, delta))\n",
    "    return(out)\n",
    "\n",
    "def EM_Q(pi, X, y, delta):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "\n",
    "model_dir_f = \"./f_log_EM\"\n",
    "\n",
    "# Fit DNN regressor\n",
    "model_dnn_f = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                            model_dir=model_dir_f)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn_f.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "# Train prediction\n",
    "train_pred = train[[\"id\"]].assign(y_hat=0, pi_hat=0)\n",
    "temp = model_dnn_f.predict(input_fn=lambda: input_fn(train))\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Fit DNN softmax\n",
    "model_dir_g = \"./g_log_EM\"\n",
    "model_dnn_g = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_g\"][0]],\n",
    "                                            model_dir=model_dir_g)\n",
    "\n",
    "model_dnn_g.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_g\"][0])\n",
    "temp = model_dnn_g.predict(input_fn=lambda: input_fn(train))\n",
    "pi_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "\n",
    "# Compute softmax\n",
    "train_pred[\"pi_hat\"] = np.exp(pi_hat)\n",
    "pi_hat_sum_dict = train_pred.groupby(\"id\")[\"pi_hat\"].sum().to_dict()\n",
    "# \"map\" is actually much faster than \"replace\"\n",
    "train_pred[\"pi_hat_sum\"] = train_pred[\"id\"].map(pi_hat_sum_dict)\n",
    "train_pred[\"pi_hat\"] /= train_pred[\"pi_hat_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# EM algorithm\n",
    "# Change to Q\n",
    "EM_decreasing = True\n",
    "nb_iteration = 0\n",
    "while (EM_decreasing):\n",
    "    nb_iteration +=1\n",
    "\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "\n",
    "    print(\"Iteration {0} EM: {1}\".format(nb_iteration, new_RMSE))\n",
    "\n",
    "    if (abs(new_RMSE - RMSE) > 0.1):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        EM_decreasing = False\n",
    "\n",
    "models = {\"dnn_1\": model_dnn}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pred = test[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "for i, m in models.items():\n",
    "    temp = m.predict(input_fn=lambda: input_fn(test))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    test_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = test_pred.groupby(\"id\").median()[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69482286537295845"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(test_pred[\"id\"].map(y_hat_med).values, test[\"y\"]))\n",
    "RMSE\n",
    "#0.65725435012348465 for Pred 4\n",
    "#0.65362864377856866 for Pred 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kaggle_pred = pd.DataFrame({\"Id\": y_hat_med.index, \"y\": y_hat_med.values})\n",
    "kaggle_pred.to_csv(\"Prediction_6.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark:\n",
    "* Submit 1 (ensemble of xgboost + 2 ridge with instances model)\n",
    "eta\teval_metric\tgamma\tlambda\tmax_depth\tmin_child_weight\tnthread\tobjective\tseed\tsilent\tsubsample\tscore\n",
    "0.91834 Public LB 300 trees 0.09\trmse\t0.2\t0.8\t4\t4.0\t-1\treg:linear\t22\t0\t0.7\t0.883339 (cross-val)\n",
    "    \n",
    "* Submit 2\n",
    "\n",
    "Pruning with linear regression\n",
    "then add contributions of aggregated xgboost + linear model\n",
    "\n",
    "0.78181 Public LB  0.779345 CV\n",
    "\n",
    "* Submit 3\n",
    "DNN pruning\n",
    "(wrong CV)\n",
    "0.73270 Public LB 0.663128 CV with DNN 10 neurons 900 steps gain_threshold = 0.01 outliers_threshold = 0.05\n",
    "\n",
    "* Prediction 4\n",
    "LB 0.74713\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.701658836521\n",
    "using fist pruning DNN\n",
    "* Prediction 5\n",
    "LB 0.74453\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.01, 'steps': 2400, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.707300896236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Dropout\n",
    "Regression\n",
    "Validation set instead of cross val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
