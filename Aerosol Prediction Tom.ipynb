{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "from hyperopt import STATUS_OK, hp, fmin, tpe, Trials, space_eval\n",
    "\n",
    "from time import time\n",
    "import operator\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "def load_data():\n",
    "    full_data = pd.read_csv(\"X.csv\")\n",
    "    train_y = pd.read_csv(\"ytr.csv\")\n",
    "    # Rename columns to something more interpretable\n",
    "    columns = ([\"reflectance_\" + str(i) for i in range(7)]\n",
    "               + [\"solar_\" + str(i) for i in range(5)] + [\"id\"])\n",
    "    full_data.columns = columns\n",
    "    # Add y to the data frame\n",
    "    split = 98000\n",
    "    y_id_dict = train_y.set_index(\"Id\")[\"y\"].to_dict()\n",
    "    full_data.loc[:(split-1), \"y\"] = full_data.loc[:(split-1), \"id\"].map(y_id_dict)\n",
    "\n",
    "    train, test = full_data[:split], full_data[split:]\n",
    "    return (train, test)\n",
    "\n",
    "#columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    "#           + [\"solar_\" + str(i) for i in range(5)] + [\"y\"])\n",
    "#full_data = pd.read_csv(\"MODIS.csv\", header=None, names=columns)\n",
    "#split = 98000\n",
    "#train, test = full_data[:split].copy(), full_data[split:].copy()\n",
    "#train_copy, test_copy = load_data()\n",
    "train, test = load_data()\n",
    "\n",
    "# Parameters\n",
    "n_threads = -1\n",
    "random_seed = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reflectance_0</th>\n",
       "      <th>reflectance_1</th>\n",
       "      <th>reflectance_2</th>\n",
       "      <th>reflectance_3</th>\n",
       "      <th>reflectance_4</th>\n",
       "      <th>reflectance_5</th>\n",
       "      <th>reflectance_6</th>\n",
       "      <th>solar_0</th>\n",
       "      <th>solar_1</th>\n",
       "      <th>solar_2</th>\n",
       "      <th>solar_3</th>\n",
       "      <th>solar_4</th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.580642</td>\n",
       "      <td>2.482233</td>\n",
       "      <td>5.887092</td>\n",
       "      <td>4.732722</td>\n",
       "      <td>4.408482</td>\n",
       "      <td>3.830171</td>\n",
       "      <td>4.388508</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.338455</td>\n",
       "      <td>3.627796</td>\n",
       "      <td>4.723716</td>\n",
       "      <td>3.324726</td>\n",
       "      <td>2.743442</td>\n",
       "      <td>4.727652</td>\n",
       "      <td>2.810193</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.224569</td>\n",
       "      <td>3.522241</td>\n",
       "      <td>6.188831</td>\n",
       "      <td>4.389783</td>\n",
       "      <td>4.177616</td>\n",
       "      <td>4.945918</td>\n",
       "      <td>4.122848</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.717218</td>\n",
       "      <td>2.712012</td>\n",
       "      <td>5.024211</td>\n",
       "      <td>3.944907</td>\n",
       "      <td>3.393424</td>\n",
       "      <td>3.931973</td>\n",
       "      <td>3.489578</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.378857</td>\n",
       "      <td>3.644976</td>\n",
       "      <td>4.515292</td>\n",
       "      <td>3.223825</td>\n",
       "      <td>2.739952</td>\n",
       "      <td>4.599662</td>\n",
       "      <td>2.781574</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reflectance_0  reflectance_1  reflectance_2  reflectance_3  reflectance_4  \\\n",
       "0       1.580642       2.482233       5.887092       4.732722       4.408482   \n",
       "1       2.338455       3.627796       4.723716       3.324726       2.743442   \n",
       "2       2.224569       3.522241       6.188831       4.389783       4.177616   \n",
       "3       1.717218       2.712012       5.024211       3.944907       3.393424   \n",
       "4       2.378857       3.644976       4.515292       3.223825       2.739952   \n",
       "\n",
       "   reflectance_5  reflectance_6    solar_0   solar_1   solar_2   solar_3  \\\n",
       "0       3.830171       4.388508  22.572888  63.58724  88.05048  4.495216   \n",
       "1       4.727652       2.810193  22.572888  63.58724  88.05048  4.495216   \n",
       "2       4.945918       4.122848  22.572888  63.58724  88.05048  4.495216   \n",
       "3       3.931973       3.489578  22.572888  63.58724  88.05048  4.495216   \n",
       "4       4.599662       2.781574  22.572888  63.58724  88.05048  4.495216   \n",
       "\n",
       "     solar_4  id         y  \n",
       "0 -50.699904   1 -3.998082  \n",
       "1 -50.699904   1 -3.998082  \n",
       "2 -50.699904   1 -3.998082  \n",
       "3 -50.699904   1 -3.998082  \n",
       "4 -50.699904   1 -3.998082  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_excl = [\"id\", \"y\"]\n",
    "cols_orig = [c for c in train.columns if c not in cols_excl]\n",
    "\n",
    "# Standardise data for LR\n",
    "train[cols_orig] = scale(train[cols_orig])\n",
    "test[cols_orig] = scale(test[cols_orig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "models_weights = {\"dnn_1\": 1.0}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "\n",
    "# Scoring function in the hyperopt hyperparameters tuning.\n",
    "def scoring_function(parameters):\n",
    "    print(\"Training the model with parameters: \")\n",
    "    print(parameters)\n",
    "    average_RMSE = 0.0\n",
    "    n_splits = 5\n",
    "    \n",
    "    # Generate random integer for model_dir\n",
    "    random_int = np.random.randint(1000)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    nb_fold = 0\n",
    "    for train_index, validation_index in kf.split(train):\n",
    "        nb_fold += 1\n",
    "        train_fold, validation_fold = train.loc[train_index], train.loc[validation_index] \n",
    "\n",
    "        feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "        model_dir = (\"./log_\"\n",
    "                     + str(parameters[\"steps\"]) + \"_\"\n",
    "                     + str(parameters[\"nb_neurons_1\"]) + \"_\"\n",
    "                     #+ str(parameters[\"nb_neurons_2\"])\n",
    "                     + str(nb_fold) + \"_\"\n",
    "                     + str(random_int)\n",
    "                    )\n",
    "        \n",
    "        # Tune number of layers\n",
    "        model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                  hidden_units=[10],\n",
    "                                                                #parameters[\"nb_neurons_2\"]],\n",
    "                                                  #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                   #   learning_rate=0.1,\n",
    "                                                    #  l1_regularization_strength=0.001),\n",
    "                                                  model_dir=model_dir)\n",
    "\n",
    "        def input_fn(data_set):\n",
    "            feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "            labels = tf.constant(data_set[\"y\"].values)\n",
    "            return feature_cols, labels\n",
    "        \n",
    "        model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=900)\n",
    "\n",
    "        train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "        #for i, m in models.items():\n",
    "        temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "        # .predict() returns an iterator; convert to an array\n",
    "        y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "        train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "        # Use median value by id\n",
    "        y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "        RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train_fold[\"y\"]))\n",
    "        \n",
    "        # Prune outliers\n",
    "        RMSE_decreasing = True\n",
    "        count = 0\n",
    "        while (RMSE_decreasing):\n",
    "            count +=1\n",
    "            train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "            # Distance from the median for each bag\n",
    "            train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "            # Rank of each instance by bag\n",
    "            train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "            bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "            train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "            train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "            # Remove outliers\n",
    "            outliers_index = train_pred[\"rank\"] > (1 - parameters[\"outliers_threshold\"])\n",
    "            train_fold = train_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "\n",
    "            model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                      hidden_units=[10],\n",
    "                                                                    #parameters[\"nb_neurons_2\"]],\n",
    "                                                      #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                       #   learning_rate=0.1,\n",
    "                                                        #  l1_regularization_strength=0.001),\n",
    "                                                      model_dir=model_dir)\n",
    "\n",
    "            model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=900)\n",
    "\n",
    "            # Compute new RMSE\n",
    "            train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            #for i, m in models.items():\n",
    "            temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "            y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "            new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train_fold[\"y\"]))\n",
    "            print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "\n",
    "            if (abs(new_RMSE - RMSE) > parameters[\"gain_threshold\"]):\n",
    "                RMSE = new_RMSE\n",
    "            else:\n",
    "                RMSE_decreasing = False\n",
    "        \n",
    "        # Second bigger DNN\n",
    "        model_dnn_1 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                    hidden_units=[parameters[\"nb_neurons_1\"]])\n",
    "                                                    #model_dir=model_dir)\n",
    "        model_dnn_1.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "        \n",
    "        # Changed to model_dnn instead of model_dnn_1\n",
    "        models = {\"dnn_1\": model_dnn_1}\n",
    "        \n",
    "        # Compute RMSE on validation set\n",
    "        validation_pred = validation_fold[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "        for i, m in models.items():\n",
    "            temp = m.predict(input_fn=lambda: input_fn(validation_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            validation_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "            \n",
    "        # Use median value by id\n",
    "        y_hat_med = validation_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "        \n",
    "        RMSE = np.sqrt(mean_squared_error(validation_pred[\"id\"].map(y_hat_med).values, validation_fold[\"y\"]))\n",
    "        average_RMSE += RMSE\n",
    "        print(\"Validation fold {0} RMSE: {1}\".format(nb_fold, RMSE))\n",
    "\n",
    "    average_RMSE /= n_splits\n",
    "\n",
    "    print(\"Cross-validation score: {0}\\n\".format(average_RMSE))\n",
    "    \n",
    "    return {\"loss\": average_RMSE, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 10000, 'nb_neurons_1': 24, 'outliers_threshold': 0.05}\n",
      "Pruning 1 RMSE: 0.699313533454\n",
      "Pruning 2 RMSE: 0.688515929788\n",
      "Pruning 3 RMSE: 0.68023760989\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-534c0de84595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m best = fmin(scoring_function, parameters_grid, algo=tpe.suggest, max_evals=1, \n\u001b[0;32m---> 14\u001b[0;31m             trials=trials)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcomputing_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c2da311c6d72>\u001b[0m in \u001b[0;36mscoring_function\u001b[0;34m(parameters)\u001b[0m\n\u001b[1;32m    111\u001b[0m                                                     hidden_units=[parameters[\"nb_neurons_1\"]])\n\u001b[1;32m    112\u001b[0m                                                     \u001b[0;31m#model_dir=model_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mmodel_dnn_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Changed to model_dnn instead of model_dnn_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 280\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    282\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_fn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m       \u001b[0msummary_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriterCache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    460\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    784\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    787\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbortedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         logging.info('An AbortedError was raised. Closing the current session. '\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    889\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Grid to pick parameters from.\n",
    "parameters_grid = {\"steps\"             : hp.choice(\"steps\", np.arange(10000, 10100, 100, dtype=int)),\n",
    "                   \"nb_neurons_1\"      : hp.choice(\"nb_neurons_1\", np.arange(24, 25, 1, dtype=int)),\n",
    "                   \"outliers_threshold\": hp.quniform(\"outliers_threshold\", 0.05, 0.051, 0.01),\n",
    "                   \"gain_threshold\"    : hp.quniform(\"gain_threshold\", 0.005, 0.01, 0.005)\n",
    "                   #\"nb_neurons_2\": hp.choice(\"nb_neurons_2\", np.arange(5, 10, 1, dtype=int))\n",
    "                  }\n",
    "# Record the information about the cross-validation.\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(scoring_function, parameters_grid, algo=tpe.suggest, max_evals=1, \n",
    "            trials=trials)\n",
    "\n",
    "computing_time = time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pruning 1 RMSE: 0.693409560231\n",
    "Pruning 2 RMSE: 0.677430813304\n",
    "Pruning 3 RMSE: 0.668280328097\n",
    "Pruning 4 RMSE: 0.662140161136\n",
    "Pruning 5 RMSE: 0.656442073871\n",
    "Pruning 6 RMSE: 0.651894112733\n",
    "Pruning 7 RMSE: 0.647910685952\n",
    "Pruning 8 RMSE: 0.644673528421\n",
    "Pruning 9 RMSE: 0.642254075294\n",
    "Validation fold 1 RMSE: 0.646948737672\n",
    "Pruning 1 RMSE: 0.711768146502\n",
    "Pruning 2 RMSE: 0.689930723516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1 DNN with model_dir corrected\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.005, 'steps': 1000, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.708254362329\n",
    "Pruning 2 RMSE: 0.690809364036\n",
    "Pruning 3 RMSE: 0.681259286244\n",
    "Pruning 4 RMSE: 0.675679762642\n",
    "Pruning 5 RMSE: 0.670965511179\n",
    "Validation fold 1 RMSE: 0.64861798817\n",
    "Pruning 1 RMSE: 0.658292571898\n",
    "Pruning 2 RMSE: 0.645737414652\n",
    "Pruning 3 RMSE: 0.63797501753\n",
    "Pruning 4 RMSE: 0.632583378549\n",
    "Pruning 5 RMSE: 0.627584029004\n",
    "Validation fold 2 RMSE: 0.723643911448\n",
    "Pruning 1 RMSE: 0.687961545764\n",
    "Pruning 2 RMSE: 0.672401033846\n",
    "Pruning 3 RMSE: 0.660611288466\n",
    "Pruning 4 RMSE: 0.650237174155\n",
    "Pruning 5 RMSE: 0.64198570573\n",
    "Pruning 6 RMSE: 0.636986731147\n",
    "Validation fold 3 RMSE: 0.719981056486\n",
    "Pruning 1 RMSE: 0.697781938911\n",
    "Pruning 2 RMSE: 0.681268268398\n",
    "Pruning 3 RMSE: 0.671483140797\n",
    "Pruning 4 RMSE: 0.666088232133\n",
    "Pruning 5 RMSE: 0.661858777702\n",
    "Validation fold 4 RMSE: 0.715854578144\n",
    "Pruning 1 RMSE: 0.66224449426\n",
    "Pruning 2 RMSE: 0.644538081642\n",
    "Pruning 3 RMSE: 0.634079022739\n",
    "Pruning 4 RMSE: 0.624630852786\n",
    "Pruning 5 RMSE: 0.619603795183\n",
    "Pruning 6 RMSE: 0.61598970139\n",
    "Validation fold 5 RMSE: 0.762927716346\n",
    "Cross-validation score: 0.714205050119\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 1100, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.713960264442\n",
    "Pruning 2 RMSE: 0.701637487701\n",
    "Pruning 3 RMSE: 0.69375349128\n",
    "Validation fold 1 RMSE: 0.641879797964\n",
    "Pruning 1 RMSE: 0.671532366166\n",
    "Pruning 2 RMSE: 0.656442799709\n",
    "Pruning 3 RMSE: 0.645330625403\n",
    "Pruning 4 RMSE: 0.638014896705\n",
    "Validation fold 2 RMSE: 0.741045867638\n",
    "Pruning 1 RMSE: 0.690110283558\n",
    "Pruning 2 RMSE: 0.678068607072\n",
    "Pruning 3 RMSE: 0.662923307388\n",
    "Pruning 4 RMSE: 0.654881538105\n",
    "Validation fold 3 RMSE: 0.687377107282\n",
    "Pruning 1 RMSE: 0.70616839345\n",
    "Pruning 2 RMSE: 0.692895897509\n",
    "Pruning 3 RMSE: 0.686021953655\n",
    "Validation fold 4 RMSE: 0.688327537634\n",
    "Pruning 1 RMSE: 0.66943420746\n",
    "Pruning 2 RMSE: 0.65286815522\n",
    "Pruning 3 RMSE: 0.638691155949\n",
    "Pruning 4 RMSE: 0.627600556022\n",
    "Pruning 5 RMSE: 0.622316478\n",
    "Validation fold 5 RMSE: 0.765910014041\n",
    "Cross-validation score: 0.704908064912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min(trials.losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save the best parameters as a csv.\n",
    "best_parameters = pd.DataFrame({key: [value] for (key, value) in \n",
    "                                zip(space_eval(parameters_grid, best).keys(),\n",
    "                                    space_eval(parameters_grid, best).values())})\n",
    "# Add the corresponding score.\n",
    "best_parameters[\"score\"] = min(trials.losses())\n",
    "best_parameters.to_csv(\"best_parameters_6.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 1 RMSE: 0.682372839094\n",
      "Pruning 2 RMSE: 0.668013398984\n",
      "Pruning 3 RMSE: 0.659537802041\n"
     ]
    }
   ],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "models_weights = {\"dnn_1\": 1.0}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "\n",
    "model_dir = \"./log_submit_6\"\n",
    "        \n",
    "# Tune number of layers\n",
    "model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                          hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                          model_dir=model_dir)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "        \n",
    "\n",
    "train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to an array\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train[\"y\"]))\n",
    "        \n",
    "# Prune outliers\n",
    "RMSE_decreasing = True\n",
    "count = 0\n",
    "while (RMSE_decreasing):\n",
    "    count +=1\n",
    "    train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "    # Distance from the median for each bag\n",
    "    train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "    # Rank of each instance by bag\n",
    "    train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "    bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "    train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "    train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    outliers_index = train_pred[\"rank\"] > (1 - best_parameters[\"outliers_threshold\"][0])\n",
    "    train = train.loc[~outliers_index, :].reset_index(drop=True)\n",
    "\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "    # Use median value by id\n",
    "    y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "    new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train[\"y\"]))\n",
    "    print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "\n",
    "    if (abs(new_RMSE - RMSE) > best_parameters[\"gain_threshold\"][0]):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        RMSE_decreasing = False\n",
    "        \n",
    "# Bagging of RNN\n",
    "\n",
    "# Bootstrap 1\n",
    "#train_1 = train\n",
    "#model_dnn_1 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    " #                                           hidden_units=[best_parameters[\"nb_neurons_1\"][0]])\n",
    "#model_dnn_1.fit(input_fn=lambda: input_fn(train_1), steps=best_parameters[\"steps\"][0])\n",
    "    \n",
    "models = {\"dnn_1\": model_dnn}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pred = test[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "for i, m in models.items():\n",
    "    temp = m.predict(input_fn=lambda: input_fn(test))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    test_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = test_pred.groupby(\"id\").median()[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#RMSE = np.sqrt(mean_squared_error(test_pred[\"id\"].map(y_hat_med).values, test[\"y\"]))\n",
    "#RMSE\n",
    "#0.65725435012348465 for Pred 4\n",
    "#0.65362864377856866 for Pred 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kaggle_pred = pd.DataFrame({\"Id\": y_hat_med.index, \"y\": y_hat_med.values})\n",
    "kaggle_pred.to_csv(\"Prediction_6.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark:\n",
    "* Submit 1 (ensemble of xgboost + 2 ridge with instances model)\n",
    "eta\teval_metric\tgamma\tlambda\tmax_depth\tmin_child_weight\tnthread\tobjective\tseed\tsilent\tsubsample\tscore\n",
    "0.91834 Public LB 300 trees 0.09\trmse\t0.2\t0.8\t4\t4.0\t-1\treg:linear\t22\t0\t0.7\t0.883339 (cross-val)\n",
    "    \n",
    "* Submit 2\n",
    "\n",
    "Pruning with linear regression\n",
    "then add contributions of aggregated xgboost + linear model\n",
    "\n",
    "0.78181 Public LB  0.779345 CV\n",
    "\n",
    "* Submit 3\n",
    "DNN pruning\n",
    "(wrong CV)\n",
    "0.73270 Public LB 0.663128 CV with DNN 10 neurons 900 steps gain_threshold = 0.01 outliers_threshold = 0.05\n",
    "\n",
    "* Prediction 4\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.701658836521\n",
    "using fist pruning DNN\n",
    "* Prediction 5\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.01, 'steps': 2400, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.707300896236"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
