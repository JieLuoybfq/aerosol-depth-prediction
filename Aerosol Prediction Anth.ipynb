{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "from hyperopt import STATUS_OK, hp, fmin, tpe, Trials, space_eval\n",
    "\n",
    "from time import time\n",
    "import operator\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "def load_data():\n",
    "    full_data = pd.read_csv(\"X.csv\")\n",
    "    train_y = pd.read_csv(\"ytr.csv\")\n",
    "    # Rename columns to something more interpretable\n",
    "    columns = ([\"reflectance_\" + str(i) for i in range(7)]\n",
    "               + [\"solar_\" + str(i) for i in range(5)] + [\"id\"])\n",
    "    full_data.columns = columns\n",
    "    # Add y to the data frame\n",
    "    split = 98000\n",
    "    y_id_dict = train_y.set_index(\"Id\")[\"y\"].to_dict()\n",
    "    full_data.loc[:(split-1), \"y\"] = full_data.loc[:(split-1), \"id\"].map(y_id_dict)\n",
    "\n",
    "    train, test = full_data[:split], full_data[split:]\n",
    "    return (train, test)\n",
    "\n",
    "#columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    " #          + [\"solar_\" + str(i) for i in range(5)] + [\"y\"])\n",
    "#full_data = pd.read_csv(\"MODIS.csv\", header=None, names=columns)\n",
    "#split = 98000\n",
    "#train, test = full_data[:split].copy(), full_data[split:].copy()\n",
    "#train = full_data.copy()\n",
    "\n",
    "#train_copy, test_copy = load_data()\n",
    "\n",
    "train, test = load_data()\n",
    "\n",
    "# Parameters\n",
    "n_threads = -1\n",
    "random_seed = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reflectance_0</th>\n",
       "      <th>reflectance_1</th>\n",
       "      <th>reflectance_2</th>\n",
       "      <th>reflectance_3</th>\n",
       "      <th>reflectance_4</th>\n",
       "      <th>reflectance_5</th>\n",
       "      <th>reflectance_6</th>\n",
       "      <th>solar_0</th>\n",
       "      <th>solar_1</th>\n",
       "      <th>solar_2</th>\n",
       "      <th>solar_3</th>\n",
       "      <th>solar_4</th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.580642</td>\n",
       "      <td>2.482233</td>\n",
       "      <td>5.887092</td>\n",
       "      <td>4.732722</td>\n",
       "      <td>4.408482</td>\n",
       "      <td>3.830171</td>\n",
       "      <td>4.388508</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.338455</td>\n",
       "      <td>3.627796</td>\n",
       "      <td>4.723716</td>\n",
       "      <td>3.324726</td>\n",
       "      <td>2.743442</td>\n",
       "      <td>4.727652</td>\n",
       "      <td>2.810193</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.224569</td>\n",
       "      <td>3.522241</td>\n",
       "      <td>6.188831</td>\n",
       "      <td>4.389783</td>\n",
       "      <td>4.177616</td>\n",
       "      <td>4.945918</td>\n",
       "      <td>4.122848</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.717218</td>\n",
       "      <td>2.712012</td>\n",
       "      <td>5.024211</td>\n",
       "      <td>3.944907</td>\n",
       "      <td>3.393424</td>\n",
       "      <td>3.931973</td>\n",
       "      <td>3.489578</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.378857</td>\n",
       "      <td>3.644976</td>\n",
       "      <td>4.515292</td>\n",
       "      <td>3.223825</td>\n",
       "      <td>2.739952</td>\n",
       "      <td>4.599662</td>\n",
       "      <td>2.781574</td>\n",
       "      <td>22.572888</td>\n",
       "      <td>63.58724</td>\n",
       "      <td>88.05048</td>\n",
       "      <td>4.495216</td>\n",
       "      <td>-50.699904</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.998082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reflectance_0  reflectance_1  reflectance_2  reflectance_3  reflectance_4  \\\n",
       "0       1.580642       2.482233       5.887092       4.732722       4.408482   \n",
       "1       2.338455       3.627796       4.723716       3.324726       2.743442   \n",
       "2       2.224569       3.522241       6.188831       4.389783       4.177616   \n",
       "3       1.717218       2.712012       5.024211       3.944907       3.393424   \n",
       "4       2.378857       3.644976       4.515292       3.223825       2.739952   \n",
       "\n",
       "   reflectance_5  reflectance_6    solar_0   solar_1   solar_2   solar_3  \\\n",
       "0       3.830171       4.388508  22.572888  63.58724  88.05048  4.495216   \n",
       "1       4.727652       2.810193  22.572888  63.58724  88.05048  4.495216   \n",
       "2       4.945918       4.122848  22.572888  63.58724  88.05048  4.495216   \n",
       "3       3.931973       3.489578  22.572888  63.58724  88.05048  4.495216   \n",
       "4       4.599662       2.781574  22.572888  63.58724  88.05048  4.495216   \n",
       "\n",
       "     solar_4  id         y  \n",
       "0 -50.699904   1 -3.998082  \n",
       "1 -50.699904   1 -3.998082  \n",
       "2 -50.699904   1 -3.998082  \n",
       "3 -50.699904   1 -3.998082  \n",
       "4 -50.699904   1 -3.998082  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_excl = [\"id\", \"y\"]\n",
    "cols_orig = [c for c in train.columns if c not in cols_excl]\n",
    "\n",
    "# Remove outliers\n",
    "#outliers_id = [21, 31, 72, 85, 135, 154, 165,\n",
    " #              199, 232, 252, 255, 262, 289, 387,\n",
    "  #             393, 404, 408, 434, 488, 516, 578,\n",
    "   #            615, 617, 624, 633, 647, 683, 778,\n",
    "    #           785, 792, 817, 828, 917, 946, 960]\n",
    "\n",
    "#train = train[~train[\"id\"].isin(outliers_id)].reset_index(drop=True)\n",
    "\n",
    "# Standardise data for LR\n",
    "train[cols_orig] = scale(train[cols_orig])\n",
    "test[cols_orig] = scale(test[cols_orig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_orig]\n",
    "\n",
    "#regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                         # hidden_units=[9, 9],\n",
    "                                         #)#model_dir=\"./temp_log\")\n",
    "\n",
    "#train, test = train_copy[19600:].copy(), train_copy[:19600].copy()\n",
    "\n",
    "#def input_fn(data_set):\n",
    " #   feature_cols = {k: tf.constant(data_set[k].values) for k in cols_orig}\n",
    "  #  labels = tf.constant(data_set[\"y\"].values)\n",
    "    \n",
    "   # return feature_cols, labels\n",
    "\n",
    "#validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    " #   input_fn=lambda: input_fn(test),\n",
    "  #  early_stopping_rounds=100)\n",
    "\n",
    "#regressor.fit(input_fn=lambda: input_fn(train), steps=10)\n",
    "              #monitors=[validation_monitor])\n",
    "\n",
    "#ev = regressor.evaluate(input_fn=lambda: input_fn(train), steps=1)\n",
    "#loss_score = ev[\"loss\"]\n",
    "#print(\"Loss: {0:f}\".format(loss_score))\n",
    "\n",
    "#y = regressor.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to a list and print predictions\n",
    "#predictions = np.array(list(itertools.islice(y, 0, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pdf_weight(data):\n",
    "    cols = list(data.columns)\n",
    "    X = data.copy()\n",
    "    \n",
    "    y_mean_dict = X.groupby(\"id\")[\"y_hat\"].mean().to_dict()\n",
    "    y_std_dict = X.groupby(\"id\")[\"y_hat\"].std().to_dict()\n",
    "    X[\"y_hat_mean\"] = X[\"id\"].map(y_mean_dict)\n",
    "    X[\"y_hat_std\"] = X[\"id\"].map(y_std_dict)\n",
    "    X[\"pdf\"] = norm.pdf(X[\"y_hat\"], X[\"y_hat_mean\"], \n",
    "                        X[\"y_hat_std\"])\n",
    "    y_pdf_sum_dict = X.groupby(\"id\")[\"pdf\"].sum().to_dict()\n",
    "    X[\"pdf_sum\"] = X[\"id\"].map(y_pdf_sum_dict)\n",
    "    X[\"pdf\"] /= X[\"pdf_sum\"]\n",
    "    X[\"y_hat_weighted\"] = X[\"y_hat\"] * X[\"pdf\"]\n",
    "    \n",
    "    y_weighted_dict = X.groupby(\"id\")[\"y_hat_weighted\"].sum().to_dict()\n",
    "    X[\"y_hat_weighted_sum\"] = X[\"id\"].map(y_weighted_dict)\n",
    "    \n",
    "    return(X[cols + [\"y_hat_weighted_sum\"]])\n",
    "\n",
    "def discard_noisy_pixels(X, reflect_col, dark_lim, bright_lim):\n",
    "    reflect_rank = X.groupby(\"id\")[reflect_col].rank()\n",
    "    index = (reflect_rank > dark_lim) & (reflect_rank < bright_lim)\n",
    "    return(X.loc[index].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols_dnn = cols_orig\n",
    "\n",
    "models_weights = {\"dnn_1\": 1}#, \"dnn_2\": 0.2, \"dnn_3\": 0.2,\n",
    "                  #\"dnn_4\": 0.2, \"dnn_5\": 0.2}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "#models_cols = {\"dnn_1\": cols_dnn, \"dnn_2\": cols_dnn, \"dnn_3\": cols_dnn,\n",
    " #              \"dnn_4\": cols_dnn, \"dnn_5\": cols_dnn}\n",
    "    \n",
    "#learning_rate = 0.1\n",
    "reflect_col = \"reflectance_3\"\n",
    "\n",
    "# Scoring function in the hyperopt hyperparameters tuning.\n",
    "def scoring_function(parameters):\n",
    "    print(\"Training the model with parameters: \")\n",
    "    print(parameters)\n",
    "    average_RMSE = 0.0\n",
    "    n_splits = 5\n",
    "    \n",
    "    # Generate random integer for model_dir\n",
    "    random_int = np.random.randint(1000)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    nb_fold = 0\n",
    "    for train_index, validation_index in kf.split(train):\n",
    "        nb_fold += 1\n",
    "        train_fold, validation_fold = train.loc[train_index], train.loc[validation_index]\n",
    "        \n",
    "        # Remove outliers\n",
    "        #train_fold = train_fold[~train_fold[\"id\"].isin(outliers_id)].reset_index(drop=True)\n",
    "        \n",
    "        # Remove darkest and brightest pixels\n",
    "        train_fold = discard_noisy_pixels(train_fold, reflect_col,\n",
    "                                          parameters[\"dark_lim\"], parameters[\"bright_lim\"])\n",
    "\n",
    "        feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "        count = 0\n",
    "        model_dir = (\"./log_\"\n",
    "                     + str(parameters[\"steps\"]) + \"_\"\n",
    "                     + str(parameters[\"nb_neurons_1\"]) + \"_\"\n",
    "                     #+ str(parameters[\"nb_neurons_2\"])\n",
    "                     + str(nb_fold) + \"_\"\n",
    "                     + str(count) + \"_\"\n",
    "                     + str(random_int)\n",
    "                    )\n",
    "        \n",
    "        # Tune number of layers\n",
    "        model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                  hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                #parameters[\"nb_neurons_2\"]],\n",
    "                                                  #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                   #   learning_rate=learning_rate,\n",
    "                                                    #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                  #dropout=parameters[\"dropout\"],\n",
    "                                                  model_dir=model_dir)\n",
    "\n",
    "        def input_fn(data_set):\n",
    "            feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "            labels = tf.constant(data_set[\"y\"].values)\n",
    "            return feature_cols, labels\n",
    "        \n",
    "        model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "        train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "        #for i, m in models.items():\n",
    "        temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "        # .predict() returns an iterator; convert to an array\n",
    "        y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "        train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "        # Use median value by id\n",
    "        y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "        RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train_fold[\"y\"]))\n",
    "        print(\"Pruning {0} RMSE: {1}\".format(count, RMSE))\n",
    "        \n",
    "        # Prune outliers\n",
    "        RMSE_decreasing = False\n",
    "        while (RMSE_decreasing):\n",
    "            count +=1\n",
    "            train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "            # Distance from the median for each bag\n",
    "            train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "            # Rank of each instance by bag\n",
    "            train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "            bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "            train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "            train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "            # Remove outliers\n",
    "            outliers_index = train_pred[\"rank\"] > (1 - parameters[\"outliers_threshold\"])\n",
    "            train_fold = train_fold.loc[~outliers_index, :].reset_index(drop=True)\n",
    "            \n",
    "            model_dir = (\"./log_\"\n",
    "                         + str(parameters[\"steps\"]) + \"_\"\n",
    "                         + str(parameters[\"nb_neurons_1\"]) + \"_\"\n",
    "                         #+ str(parameters[\"nb_neurons_2\"])\n",
    "                         + str(nb_fold) + \"_\"\n",
    "                         + str(count) + \"_\"\n",
    "                         + str(random_int)\n",
    "                        )\n",
    "\n",
    "            model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                                      hidden_units=[parameters[\"nb_neurons_1\"]],\n",
    "                                                                    #parameters[\"nb_neurons_2\"]],\n",
    "                                                      #optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                                       #   learning_rate=learning_rate,\n",
    "                                                        #  l2_regularization_strength=parameters[\"l2_reg\"]),\n",
    "                                                      #dropout=parameters[\"dropout\"],\n",
    "                                                      model_dir=model_dir)\n",
    "\n",
    "            model_dnn.fit(input_fn=lambda: input_fn(train_fold), steps=parameters[\"steps\"])\n",
    "\n",
    "            # Compute new RMSE\n",
    "            train_pred = train_fold[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "            #for i, m in models.items():\n",
    "            temp = model_dnn.predict(input_fn=lambda: input_fn(train_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "            # Use median value by id\n",
    "            y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "            new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train_fold[\"y\"]))\n",
    "            print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "            \n",
    "            if (abs(new_RMSE - RMSE) > parameters[\"gain_threshold\"]):\n",
    "            # 5 iterations of pruning\n",
    "            #if (count < 5):\n",
    "                RMSE = new_RMSE\n",
    "            else:\n",
    "                RMSE_decreasing = False\n",
    "        \n",
    "        # Changed to model_dnn instead of model_dnn_1\n",
    "        models = {\"dnn_1\": model_dnn}\n",
    "        \n",
    "        # Compute RMSE on validation set\n",
    "        validation_pred = validation_fold[[\"id\", \"y\", reflect_col]].assign(y_hat=0).reset_index(drop=True)\n",
    "        for i, m in models.items():\n",
    "            temp = m.predict(input_fn=lambda: input_fn(validation_fold))\n",
    "            # .predict() returns an iterator; convert to an array\n",
    "            y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "            validation_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "        \n",
    "        # Discard 20% darkest pixels and 50% brightest\n",
    "        validation_pred = discard_noisy_pixels(validation_pred, reflect_col,\n",
    "                                               parameters[\"dark_lim\"], parameters[\"bright_lim\"])\n",
    "        \n",
    "        # Weight each instance by gaussian pdf\n",
    "        #validation_pred = pdf_weight(validation_pred)\n",
    "        #RMSE = np.sqrt(mean_squared_error(validation_pred[\"y_hat_weighted_sum\"], validation_pred[\"y\"]))\n",
    "        \n",
    "        # Use median value by id\n",
    "        y_hat_med = validation_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "        RMSE = np.sqrt(mean_squared_error(validation_fold[\"id\"].map(y_hat_med).values, validation_fold[\"y\"]))\n",
    "        \n",
    "        average_RMSE += RMSE\n",
    "        print(\"Validation fold {0} RMSE: {1}\".format(nb_fold, RMSE))\n",
    "\n",
    "    average_RMSE /= n_splits\n",
    "\n",
    "    print(\"Cross-validation score: {0}\\n\".format(average_RMSE))\n",
    "    \n",
    "    return {\"loss\": average_RMSE, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.015, 'steps': 1800, 'bright_lim': 57, 'dark_lim': 7, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.707926010804\n",
      "Validation fold 1 RMSE: 0.833349159011\n",
      "Pruning 0 RMSE: 0.666755427479\n",
      "Validation fold 2 RMSE: 0.731854603826\n",
      "Pruning 0 RMSE: 0.693536447791\n",
      "Validation fold 3 RMSE: 0.733400432007\n",
      "Pruning 0 RMSE: 0.68043379367\n",
      "Validation fold 4 RMSE: 0.677572193735\n",
      "Pruning 0 RMSE: 0.67317091118\n",
      "Validation fold 5 RMSE: 0.761922833772\n",
      "Cross-validation score: 0.74761984447\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 1900, 'bright_lim': 45, 'dark_lim': 5, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.679712874649\n",
      "Validation fold 1 RMSE: 0.820366717526\n",
      "Pruning 0 RMSE: 0.655368022442\n",
      "Validation fold 2 RMSE: 0.749181939289\n",
      "Pruning 0 RMSE: 0.664312444281\n",
      "Validation fold 3 RMSE: 0.714015154714\n",
      "Pruning 0 RMSE: 0.671914445365\n",
      "Validation fold 4 RMSE: 0.677189442398\n",
      "Pruning 0 RMSE: 0.641138850924\n",
      "Validation fold 5 RMSE: 0.758379216149\n",
      "Cross-validation score: 0.743826494015\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 2100, 'bright_lim': 57, 'dark_lim': 7, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.67419182005\n",
      "Validation fold 1 RMSE: 0.650148217542\n",
      "Pruning 0 RMSE: 0.651840457434\n",
      "Validation fold 2 RMSE: 0.727898170043\n",
      "Pruning 0 RMSE: 0.659133823326\n",
      "Validation fold 3 RMSE: 0.714843683107\n",
      "Pruning 0 RMSE: 0.695254372049\n",
      "Validation fold 4 RMSE: 0.681609641866\n",
      "Pruning 0 RMSE: 0.639468681365\n",
      "Validation fold 5 RMSE: 0.774464174696\n",
      "Cross-validation score: 0.709792777451\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 1800, 'bright_lim': 48, 'dark_lim': 9, 'nb_neurons_1': 8, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.672513165325\n",
      "Validation fold 1 RMSE: 0.656038612467\n",
      "Pruning 0 RMSE: 0.706150254157\n",
      "Validation fold 2 RMSE: 0.7682836932\n",
      "Pruning 0 RMSE: 0.662901742045\n",
      "Validation fold 3 RMSE: 0.732467334083\n",
      "Pruning 0 RMSE: 0.694434442671\n",
      "Validation fold 4 RMSE: 0.714892761058\n",
      "Pruning 0 RMSE: 0.670039355592\n",
      "Validation fold 5 RMSE: 0.800087977702\n",
      "Cross-validation score: 0.734354075702\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.015, 'steps': 2600, 'bright_lim': 48, 'dark_lim': 7, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.679780214083\n",
      "Validation fold 1 RMSE: 0.657082186036\n",
      "Pruning 0 RMSE: 0.660203055758\n",
      "Validation fold 2 RMSE: 0.770954082902\n",
      "Pruning 0 RMSE: 0.664303535012\n",
      "Validation fold 3 RMSE: 0.712388037628\n",
      "Pruning 0 RMSE: 0.68337757934\n",
      "Validation fold 4 RMSE: 0.673689789403\n",
      "Pruning 0 RMSE: 0.651931724909\n",
      "Validation fold 5 RMSE: 0.77348311451\n",
      "Cross-validation score: 0.717519442096\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.015, 'steps': 2700, 'bright_lim': 54, 'dark_lim': 9, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.679194851364\n",
      "Validation fold 1 RMSE: 0.70515260012\n",
      "Pruning 0 RMSE: 0.659295084295\n",
      "Validation fold 2 RMSE: 0.745355112793\n",
      "Pruning 0 RMSE: 0.679025604969\n",
      "Validation fold 3 RMSE: 0.740740144063\n",
      "Pruning 0 RMSE: 0.670282523339\n",
      "Validation fold 4 RMSE: 0.683782662852\n",
      "Pruning 0 RMSE: 0.656964785241\n",
      "Validation fold 5 RMSE: 0.761265741687\n",
      "Cross-validation score: 0.727259252303\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 2600, 'bright_lim': 57, 'dark_lim': 5, 'nb_neurons_1': 8, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.693543961865\n",
      "Validation fold 1 RMSE: 0.730689085115\n",
      "Pruning 0 RMSE: 0.667444023918\n",
      "Validation fold 2 RMSE: 0.731196353683\n",
      "Pruning 0 RMSE: 0.654019049108\n",
      "Validation fold 3 RMSE: 0.726943825674\n",
      "Pruning 0 RMSE: 0.681438127102\n",
      "Validation fold 4 RMSE: 0.701208406076\n",
      "Pruning 0 RMSE: 0.660405006553\n",
      "Validation fold 5 RMSE: 0.774367880525\n",
      "Cross-validation score: 0.732881110215\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 1900, 'bright_lim': 57, 'dark_lim': 9, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.689162482943\n",
      "Validation fold 1 RMSE: 0.737989295456\n",
      "Pruning 0 RMSE: 0.664551948401\n",
      "Validation fold 2 RMSE: 0.721398902716\n",
      "Pruning 0 RMSE: 0.663736624872\n",
      "Validation fold 3 RMSE: 0.711270907231\n",
      "Pruning 0 RMSE: 0.709927494052\n",
      "Validation fold 4 RMSE: 0.724247262999\n",
      "Pruning 0 RMSE: 0.690026703131\n",
      "Validation fold 5 RMSE: 0.80208203662\n",
      "Cross-validation score: 0.739397681004\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.015, 'steps': 2400, 'bright_lim': 45, 'dark_lim': 5, 'nb_neurons_1': 10, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.685989203974\n",
      "Validation fold 1 RMSE: 0.775761768573\n",
      "Pruning 0 RMSE: 0.65498291771\n",
      "Validation fold 2 RMSE: 0.761346933895\n",
      "Pruning 0 RMSE: 0.65732106422\n",
      "Validation fold 3 RMSE: 0.704939136063\n",
      "Pruning 0 RMSE: 0.676892241744\n",
      "Validation fold 4 RMSE: 0.679980176837\n",
      "Pruning 0 RMSE: 0.642801625414\n",
      "Validation fold 5 RMSE: 0.756640864338\n",
      "Cross-validation score: 0.735733775941\n",
      "\n",
      "Training the model with parameters: \n",
      "{'gain_threshold': 0.01, 'steps': 1900, 'bright_lim': 51, 'dark_lim': 11, 'nb_neurons_1': 8, 'outliers_threshold': 0.05}\n",
      "Pruning 0 RMSE: 0.691469818636\n",
      "Validation fold 1 RMSE: 0.667080987467\n",
      "Pruning 0 RMSE: 0.670785910848\n",
      "Validation fold 2 RMSE: 0.742071879097\n",
      "Pruning 0 RMSE: 0.673442034013\n",
      "Validation fold 3 RMSE: 0.738845449572\n",
      "Pruning 0 RMSE: 0.684992940375\n",
      "Validation fold 4 RMSE: 0.673511523882\n",
      "Pruning 0 RMSE: 0.660115929797\n",
      "Validation fold 5 RMSE: 0.760862718502\n",
      "Cross-validation score: 0.716474511704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Grid to pick parameters from.\n",
    "parameters_grid = {\"steps\"             : hp.choice(\"steps\", np.arange(1500, 3000, 100, dtype=int)),\n",
    "                   \"nb_neurons_1\"      : hp.choice(\"nb_neurons_1\", np.arange(8, 11, 1, dtype=int)),\n",
    "                   \"outliers_threshold\": hp.quniform(\"outliers_threshold\", 0.05, 0.051, 0.01),\n",
    "                   \"gain_threshold\"    : hp.quniform(\"gain_threshold\", 0.01, 0.015, 0.005),\n",
    "                   \"dark_lim\"      : hp.choice(\"dark_lim\", np.arange(5, 15, 2, dtype=int)),\n",
    "                   \"bright_lim\"      : hp.choice(\"bright_lim\", np.arange(45, 65, 3, dtype=int))\n",
    "                   #\"dropout\": hp.quniform(\"dropout\", 0.2, 0.4, 0.1)\n",
    "                   #\"l2_reg\": hp.quniform(\"l2_reg\", 0.00, 0.005, 0.01)\n",
    "                   #\"nb_neurons_2\": hp.choice(\"nb_neurons_2\", np.arange(5, 10, 1, dtype=int))\n",
    "                  }\n",
    "# Record the information about the cross-validation.\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(scoring_function, parameters_grid, algo=tpe.suggest, max_evals=10, \n",
    "            trials=trials)\n",
    "\n",
    "computing_time = time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'bright_lim': 57, 'dark_lim': 5, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 0 RMSE: 0.676104764855\n",
    "Validation fold 1 RMSE: 0.665041688234\n",
    "Pruning 0 RMSE: 0.656412476559\n",
    "Validation fold 2 RMSE: 0.748346517177\n",
    "Pruning 0 RMSE: 0.68494431793\n",
    "Validation fold 3 RMSE: 0.727735868085\n",
    "Pruning 0 RMSE: 0.666319748271\n",
    "Validation fold 4 RMSE: 0.689188331182\n",
    "Pruning 0 RMSE: 0.648822767318\n",
    "Validation fold 5 RMSE: 0.781039517149\n",
    "Cross-validation score: 0.722270384365\n",
    "    \n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 3800, 'bright_lim': 57, 'dark_lim': 5, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 0 RMSE: 0.662375939065\n",
    "Validation fold 1 RMSE: 0.643900356172\n",
    "Pruning 0 RMSE: 0.652475471155\n",
    "Validation fold 2 RMSE: 0.734333964908\n",
    "Pruning 0 RMSE: 0.657422623728\n",
    "Validation fold 3 RMSE: 0.724885333197\n",
    "Pruning 0 RMSE: 0.668428420927\n",
    "Validation fold 4 RMSE: 0.660678209034\n",
    "Pruning 0 RMSE: 0.641852729063\n",
    "Validation fold 5 RMSE: 0.766109689459\n",
    "Cross-validation score: 0.705981510554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Discard darkest and brightest pixels median 10% 50% reflectance_3\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 3700, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 0 RMSE: 0.689159277446\n",
    "Validation fold 1 RMSE: 0.655020591793\n",
    "Pruning 0 RMSE: 0.651604181078\n",
    "Validation fold 2 RMSE: 0.742418209554\n",
    "Pruning 0 RMSE: 0.642671871749\n",
    "Validation fold 3 RMSE: 0.720319036944\n",
    "Pruning 0 RMSE: 0.67571478665\n",
    "Validation fold 4 RMSE: 0.688608452945\n",
    "Pruning 0 RMSE: 0.64606435882\n",
    "Validation fold 5 RMSE: 0.758060023685\n",
    "Cross-validation score: 0.712885262984\n",
    "\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 3700, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 0 RMSE: 0.665400041022\n",
    "Validation fold 1 RMSE: 0.660441234798\n",
    "Pruning 0 RMSE: 0.654655116457\n",
    "Validation fold 2 RMSE: 0.742938722534\n",
    "Pruning 0 RMSE: 0.698126608064\n",
    "Validation fold 3 RMSE: 0.745117735753\n",
    "Pruning 0 RMSE: 0.661949196442\n",
    "Validation fold 4 RMSE: 0.673310937534\n",
    "Pruning 0 RMSE: 0.634165512306\n",
    "Validation fold 5 RMSE: 0.757140157901\n",
    "Cross-validation score: 0.715789757704\n",
    "\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 3700, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 0 RMSE: 0.679150582438\n",
    "Validation fold 1 RMSE: 0.802537439314\n",
    "Pruning 0 RMSE: 0.629692451165\n",
    "Validation fold 2 RMSE: 0.727601475198\n",
    "Pruning 0 RMSE: 0.655218441308\n",
    "Validation fold 3 RMSE: 0.709988029457\n",
    "Pruning 0 RMSE: 0.668796556874\n",
    "Validation fold 4 RMSE: 0.67992545289\n",
    "Pruning 0 RMSE: 0.636853380555\n",
    "Validation fold 5 RMSE: 0.758553027167\n",
    "Cross-validation score: 0.735721084805\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 2000, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 0 RMSE: 0.670019304715\n",
    "Validation fold 1 RMSE: 0.669165593689\n",
    "Pruning 0 RMSE: 0.685869124116\n",
    "Validation fold 2 RMSE: 0.766783748327\n",
    "Pruning 0 RMSE: 0.653148077411\n",
    "Validation fold 3 RMSE: 0.736898856297\n",
    "Pruning 0 RMSE: 0.676924728634\n",
    "Validation fold 4 RMSE: 0.673257464038\n",
    "Pruning 0 RMSE: 0.653467276753\n",
    "Validation fold 5 RMSE: 0.763890032813\n",
    "Cross-validation score: 0.721999139033\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 900, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Validation fold 1 RMSE: 0.667057389166\n",
    "Validation fold 2 RMSE: 0.745337904245\n",
    "Validation fold 3 RMSE: 0.720320570752\n",
    "Validation fold 4 RMSE: 0.709991037768\n",
    "Validation fold 5 RMSE: 0.765899342935\n",
    "Cross-validation score: 0.721721248973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODIS\n",
    "Discard darkest and brightest pixels median 10% 50% reflectance_2\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 900, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Validation fold 1 RMSE: 0.0927701092668\n",
    "Validation fold 2 RMSE: 0.179845503027\n",
    "Validation fold 3 RMSE: 0.113763215729\n",
    "Validation fold 4 RMSE: 0.0987231631294\n",
    "Validation fold 5 RMSE: 0.160676485502\n",
    "Cross-validation score: 0.129155695331\n",
    "\n",
    "Discard darkest and brightest pixels median 20% 50% reflectance_0\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 900, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Validation fold 1 RMSE: 0.112719750528\n",
    "Validation fold 2 RMSE: 0.141518078026\n",
    "Validation fold 3 RMSE: 0.13111356705\n",
    "Validation fold 4 RMSE: 0.117360147513\n",
    "Validation fold 5 RMSE: 0.212927251907\n",
    "Cross-validation score: 0.143127759005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MODIS train: 980 bags\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 900, 'nb_neurons_1': 9, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.105343361018\n",
    "Validation fold 1 RMSE: 0.0987015791108\n",
    "Pruning 1 RMSE: 0.100121169819\n",
    "Pruning 2 RMSE: 0.0939525682587\n",
    "Validation fold 2 RMSE: 0.139847916988\n",
    "Pruning 1 RMSE: 0.121320922084\n",
    "Pruning 2 RMSE: 0.111532420256\n",
    "Validation fold 3 RMSE: 0.13168961676\n",
    "Pruning 1 RMSE: 0.10949595882\n",
    "Pruning 2 RMSE: 0.103120828612\n",
    "Validation fold 4 RMSE: 0.107883013046\n",
    "Pruning 1 RMSE: 0.112995428702\n",
    "Validation fold 5 RMSE: 0.188379683693\n",
    "Cross-validation score: 0.13330036192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MODIS train: 1364 bags\n",
    "    \n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.015, 'steps': 900, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.119817931796\n",
    "Validation fold 1 RMSE: 0.111377722376\n",
    "Pruning 1 RMSE: 0.112396098948\n",
    "Pruning 2 RMSE: 0.101808581135\n",
    "Validation fold 2 RMSE: 0.148817162914\n",
    "Pruning 1 RMSE: 0.115222994126\n",
    "Pruning 2 RMSE: 0.108615616905\n",
    "Validation fold 3 RMSE: 0.110455499827\n",
    "Pruning 1 RMSE: 0.111609881409\n",
    "Validation fold 4 RMSE: 0.158362010857\n",
    "Pruning 1 RMSE: 0.108284906653\n",
    "Validation fold 5 RMSE: 0.120915922243\n",
    "Cross-validation score: 0.129985663643\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "1 DNN\n",
    "\n",
    "Training the model with parameters: \n",
    "{'gain_threshold': 0.01, 'steps': 900, 'nb_neurons_1': 11, 'outliers_threshold': 0.05}\n",
    "Pruning 1 RMSE: 0.720018452178\n",
    "Pruning 2 RMSE: 0.694637808863\n",
    "Pruning 3 RMSE: 0.682604548885\n",
    "Pruning 4 RMSE: 0.675240927353\n",
    "Validation fold 1 RMSE: 0.657330001629\n",
    "Pruning 1 RMSE: 0.67466681128\n",
    "Pruning 2 RMSE: 0.65951277037\n",
    "Pruning 3 RMSE: 0.649785148888\n",
    "Validation fold 2 RMSE: 0.748604123648\n",
    "Pruning 1 RMSE: 0.68920733019\n",
    "Pruning 2 RMSE: 0.673928761686\n",
    "Pruning 3 RMSE: 0.663794898173\n",
    "Pruning 4 RMSE: 0.656852425765\n",
    "Validation fold 3 RMSE: 0.75238232566\n",
    "Pruning 1 RMSE: 0.695945822399\n",
    "Pruning 2 RMSE: 0.677436347483\n",
    "Pruning 3 RMSE: 0.667360192164\n",
    "Pruning 4 RMSE: 0.661270048569\n",
    "Validation fold 4 RMSE: 0.703611985649\n",
    "Pruning 1 RMSE: 0.675491954494\n",
    "Pruning 2 RMSE: 0.654833366872\n",
    "Pruning 3 RMSE: 0.641692836958\n",
    "Pruning 4 RMSE: 0.633366415513\n",
    "Validation fold 5 RMSE: 0.784596896669\n",
    "Cross-validation score: 0.729305066651\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6523287945244072"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(trials.losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gain_threshold</th>\n",
       "      <th>nb_neurons_1</th>\n",
       "      <th>outliers_threshold</th>\n",
       "      <th>steps</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0075</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>900</td>\n",
       "      <td>0.652329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gain_threshold  nb_neurons_1  outliers_threshold  steps     score\n",
       "0          0.0075            10                0.05    900  0.652329"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best parameters as a csv.\n",
    "best_parameters = pd.DataFrame({key: [value] for (key, value) in \n",
    "                                zip(space_eval(parameters_grid, best).keys(),\n",
    "                                    space_eval(parameters_grid, best).values())})\n",
    "# Add the corresponding score.\n",
    "best_parameters[\"score\"] = min(trials.losses())\n",
    "best_parameters.to_csv(\"best_parameters_7.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 1 RMSE: 0.623854414971\n",
      "Pruning 2 RMSE: 0.612112933147\n",
      "Pruning 3 RMSE: 0.605667733108\n"
     ]
    }
   ],
   "source": [
    "cols_dnn = cols_orig\n",
    "models_weights = {\"dnn_1\": 1.0}\n",
    "models_cols = {\"dnn_1\": cols_dnn}\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "model_dir = \"./log_submit_6\"\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "        \n",
    "# Tune number of layers\n",
    "model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                          hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                          model_dir=model_dir)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "        \n",
    "train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "# .predict() returns an iterator; convert to an array\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med).values, train[\"y\"]))\n",
    "        \n",
    "# Prune outliers\n",
    "RMSE_decreasing = True\n",
    "count = 0\n",
    "while (RMSE_decreasing):\n",
    "    count += 1\n",
    "    train_pred[\"y_med\"] = train_pred[\"id\"].map(y_hat_med)\n",
    "\n",
    "    # Distance from the median for each bag\n",
    "    train_pred[\"score\"] = (train_pred[\"y_hat\"] - train_pred[\"y_med\"])**2\n",
    "    # Rank of each instance by bag\n",
    "    train_pred[\"rank\"] = train_pred.groupby(\"id\")[\"score\"].rank()\n",
    "    bag_size_dict = train_pred.groupby(\"id\")[\"score\"].count().to_dict()\n",
    "    train_pred[\"bag_size\"] = train_pred[\"id\"].map(bag_size_dict)\n",
    "    train_pred[\"rank\"] = train_pred[\"rank\"] / train_pred[\"bag_size\"]\n",
    "\n",
    "    # Remove outliers\n",
    "    outliers_index = train_pred[\"rank\"] > (1 - best_parameters[\"outliers_threshold\"][0])\n",
    "    train = train.loc[~outliers_index, :].reset_index(drop=True)\n",
    "\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_1\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "    # Use median value by id\n",
    "    y_hat_med = train_pred.groupby(\"id\").median()[\"y_hat\"].to_dict()\n",
    "\n",
    "    new_RMSE = np.sqrt(mean_squared_error(train_pred[\"id\"].map(y_hat_med), train[\"y\"]))\n",
    "    print(\"Pruning {0} RMSE: {1}\".format(count, new_RMSE))\n",
    "\n",
    "    if (abs(new_RMSE - RMSE) > best_parameters[\"gain_threshold\"][0]):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        RMSE_decreasing = False\n",
    "        \n",
    "# Training model\n",
    "model_dnn_1 = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_1\"][0]])\n",
    "model_dnn_1.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps\"][0])\n",
    "\n",
    "models = {\"dnn_1\": model_dnn_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pred = test[[\"id\"]].assign(y_hat=0).reset_index(drop=True)\n",
    "for i, m in models.items():\n",
    "    temp = m.predict(input_fn=lambda: input_fn(test))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    test_pred[\"y_hat\"] += models_weights[i] * y_hat\n",
    "\n",
    "# Use median value by id\n",
    "y_hat_med = test_pred.groupby(\"id\").median()[\"y_hat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71423164615804291"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE = np.sqrt(mean_squared_error(test_pred[\"id\"].map(y_hat_med).values, test[\"y\"]))\n",
    "RMSE\n",
    "#0.65725435012348465 for Pred 4\n",
    "#0.65362864377856866 for Pred 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kaggle_pred = pd.DataFrame({\"Id\": y_hat_med.index, \"y\": y_hat_med.values})\n",
    "kaggle_pred.to_csv(\"Prediction_6.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark:\n",
    "* Submit 1 (ensemble of xgboost + 2 ridge with instances model)\n",
    "eta\teval_metric\tgamma\tlambda\tmax_depth\tmin_child_weight\tnthread\tobjective\tseed\tsilent\tsubsample\tscore\n",
    "0.91834 Public LB 300 trees 0.09\trmse\t0.2\t0.8\t4\t4.0\t-1\treg:linear\t22\t0\t0.7\t0.883339 (cross-val)\n",
    "    \n",
    "* Submit 2\n",
    "\n",
    "Pruning with linear regression\n",
    "then add contributions of aggregated xgboost + linear model\n",
    "\n",
    "0.78181 Public LB  0.779345 CV\n",
    "\n",
    "* Submit 3\n",
    "DNN pruning\n",
    "(wrong CV)\n",
    "0.73270 Public LB 0.663128 CV with DNN 10 neurons 900 steps gain_threshold = 0.01 outliers_threshold = 0.05\n",
    "\n",
    "* Prediction 4\n",
    "LB 0.74713\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.015, 'steps': 3800, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.701658836521\n",
    "using fist pruning DNN\n",
    "* Prediction 5\n",
    "LB 0.74453\n",
    "Ensemble of 5 DNN\n",
    "{'gain_threshold': 0.01, 'steps': 2400, 'nb_neurons_1': 10, 'outliers_threshold': 0.05} CV 0.707300896236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Dropout\n",
    "L2 regu\n",
    "Validation set instead of cross val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gamma_i_j(j, pi_i, X_i, y_i, delta):\n",
    "    out = 0.0\n",
    "    out = pi_i[j] * norm.pdf(y_i, X_i[j], delta)\n",
    "    out /= sum(pi_i * norm.pdf(y_i, X_i, delta))\n",
    "    return(out)\n",
    "\n",
    "def EM_Q(pi, X, y, delta):\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "cols_dnn = cols_orig\n",
    "\n",
    "best_parameters = pd.read_csv(\"best_parameters_6.csv\", encoding=\"utf-8\")\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in cols_dnn]\n",
    "\n",
    "model_dir_f = \"./log_submit_6\"\n",
    "\n",
    "# Fit DNN regressor\n",
    "model_dnn_f = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                            model_dir=model_dir_f)\n",
    "\n",
    "def input_fn(data_set):\n",
    "    feature_cols = {k: tf.constant(data_set[k].values) for k in cols_dnn}\n",
    "    labels = tf.constant(data_set[\"y\"].values)\n",
    "    return feature_cols, labels\n",
    "\n",
    "model_dnn_f.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "# Train prediction\n",
    "train_pred = train[[\"id\"]].assign(y_hat=0, pi_hat=0)\n",
    "temp = model_dnn_f.predict(input_fn=lambda: input_fn(train))\n",
    "y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "# Fit DNN softmax\n",
    "model_dir_g = \"./g_log_EM\"\n",
    "model_dnn_g = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                            hidden_units=[best_parameters[\"nb_neurons_g\"][0]],\n",
    "                                            model_dir=model_dir_g)\n",
    "\n",
    "model_dnn_g.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_g\"][0])\n",
    "temp = model_dnn_g.predict(input_fn=lambda: input_fn(train))\n",
    "pi_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "\n",
    "# Compute softmax\n",
    "train_pred[\"pi_hat\"] = np.exp(pi_hat)\n",
    "pi_hat_sum_dict = train_pred.groupby(\"id\")[\"pi_hat\"].sum().to_dict()\n",
    "# \"map\" is actually much faster than \"replace\"\n",
    "train_pred[\"pi_hat_sum\"] = train_pred[\"id\"].map(pi_hat_sum_dict)\n",
    "train_pred[\"pi_hat\"] /= train_pred[\"pi_hat_sum\"]\n",
    "\n",
    "# EM algorithm\n",
    "# Change to Q\n",
    "EM_decreasing = True\n",
    "nb_iteration = 0\n",
    "while (EM_decreasing):\n",
    "    nb_iteration +=1\n",
    "\n",
    "    model_dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                              hidden_units=[best_parameters[\"nb_neurons_f\"][0]],\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    model_dnn.fit(input_fn=lambda: input_fn(train), steps=best_parameters[\"steps_f\"][0])\n",
    "\n",
    "    # Compute new RMSE\n",
    "    train_pred = train[[\"id\"]].assign(y_hat=0)\n",
    "            \n",
    "    temp = model_dnn.predict(input_fn=lambda: input_fn(train))\n",
    "    # .predict() returns an iterator; convert to an array\n",
    "    y_hat = np.array(list(itertools.islice(temp, 0, None)))\n",
    "    train_pred[\"y_hat\"] = y_hat\n",
    "\n",
    "\n",
    "    print(\"Iteration {0} EM: {1}\".format(nb_iteration, new_RMSE))\n",
    "\n",
    "    if (abs(new_RMSE - RMSE) > 0.1):\n",
    "        RMSE = new_RMSE\n",
    "    else:\n",
    "        EM_decreasing = False\n",
    "\n",
    "models = {\"dnn_1\": model_dnn}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
